2023-04-10 11:32:02,471 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
2023-04-10 11:32:02,513 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
2023-04-10 11:32:02,587 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
2023-04-10 11:32:02,634 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
2023-04-10 11:32:02,682 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
2023-04-10 11:32:02,747 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
2023-04-10 11:32:02,794 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
2023-04-10 11:32:02,798 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
2023-04-10 11:32:02,799 - torch.distributed.distributed_c10d: [    INFO] - Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 11:32:02,800 - torch.distributed.distributed_c10d: [    INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 11:32:02,800 - torch.distributed.distributed_c10d: [    INFO] - Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 11:32:02,800 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 0, total 8.
2023-04-10 11:32:02,800 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 2, total 8.
2023-04-10 11:32:02,801 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 3, total 8.
2023-04-10 11:32:02,802 - torch.distributed.distributed_c10d: [    INFO] - Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 11:32:02,802 - torch.distributed.distributed_c10d: [    INFO] - Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 11:32:02,803 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 1, total 8.
2023-04-10 11:32:02,803 - torch.distributed.distributed_c10d: [    INFO] - Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 11:32:02,803 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 4, total 8.
2023-04-10 11:32:02,804 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 5, total 8.
2023-04-10 11:32:02,805 - torch.distributed.distributed_c10d: [    INFO] - Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 11:32:02,805 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 6, total 8.
2023-04-10 11:32:02,806 - torch.distributed.distributed_c10d: [    INFO] - Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 11:32:02,807 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 7, total 8.
2023-04-10 11:57:29,554 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
2023-04-10 11:57:29,685 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
2023-04-10 11:57:29,763 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
2023-04-10 11:57:29,809 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
2023-04-10 11:57:29,858 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
2023-04-10 11:57:29,926 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
2023-04-10 11:57:29,966 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
2023-04-10 11:57:29,974 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
2023-04-10 11:57:29,974 - torch.distributed.distributed_c10d: [    INFO] - Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 11:57:29,976 - torch.distributed.distributed_c10d: [    INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 11:57:29,976 - torch.distributed.distributed_c10d: [    INFO] - Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 11:57:29,976 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 0, total 8.
2023-04-10 11:57:29,977 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 4, total 8.
2023-04-10 11:57:29,977 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 7, total 8.
2023-04-10 11:57:29,977 - torch.distributed.distributed_c10d: [    INFO] - Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 11:57:29,978 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 6, total 8.
2023-04-10 11:57:29,978 - torch.distributed.distributed_c10d: [    INFO] - Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 11:57:29,979 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 5, total 8.
2023-04-10 11:57:29,980 - torch.distributed.distributed_c10d: [    INFO] - Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 11:57:29,981 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 2, total 8.
2023-04-10 11:57:29,982 - torch.distributed.distributed_c10d: [    INFO] - Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 11:57:29,983 - torch.distributed.distributed_c10d: [    INFO] - Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 11:57:29,983 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 1, total 8.
2023-04-10 11:57:29,984 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 3, total 8.
2023-04-10 12:03:18,445 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
2023-04-10 12:03:18,524 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
2023-04-10 12:03:18,574 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
2023-04-10 12:03:18,676 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
2023-04-10 12:03:18,708 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
2023-04-10 12:03:18,761 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
2023-04-10 12:03:18,789 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
2023-04-10 12:03:18,792 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
2023-04-10 12:03:18,793 - torch.distributed.distributed_c10d: [    INFO] - Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 12:03:18,793 - torch.distributed.distributed_c10d: [    INFO] - Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 12:03:18,794 - torch.distributed.distributed_c10d: [    INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 12:03:18,795 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 0, total 8.
2023-04-10 12:03:18,795 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 6, total 8.
2023-04-10 12:03:18,795 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 1, total 8.
2023-04-10 12:03:18,796 - torch.distributed.distributed_c10d: [    INFO] - Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 12:03:18,797 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 5, total 8.
2023-04-10 12:03:18,800 - torch.distributed.distributed_c10d: [    INFO] - Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 12:03:18,801 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 4, total 8.
2023-04-10 12:03:18,801 - torch.distributed.distributed_c10d: [    INFO] - Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 12:03:18,802 - torch.distributed.distributed_c10d: [    INFO] - Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 12:03:18,802 - torch.distributed.distributed_c10d: [    INFO] - Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 12:03:18,802 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 3, total 8.
2023-04-10 12:03:18,803 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 2, total 8.
2023-04-10 12:03:18,803 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 7, total 8.
2023-04-10 14:03:46,715 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
2023-04-10 14:03:46,780 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
2023-04-10 14:03:46,839 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
2023-04-10 14:03:46,889 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
2023-04-10 14:03:46,936 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
2023-04-10 14:03:46,997 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
2023-04-10 14:03:47,031 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
2023-04-10 14:03:47,041 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
2023-04-10 14:03:47,042 - torch.distributed.distributed_c10d: [    INFO] - Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 14:03:47,042 - torch.distributed.distributed_c10d: [    INFO] - Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 14:03:47,044 - torch.distributed.distributed_c10d: [    INFO] - Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 14:03:47,047 - torch.distributed.distributed_c10d: [    INFO] - Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 14:03:47,048 - torch.distributed.distributed_c10d: [    INFO] - Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 14:03:47,049 - torch.distributed.distributed_c10d: [    INFO] - Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 14:03:47,049 - torch.distributed.distributed_c10d: [    INFO] - Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 14:03:47,050 - torch.distributed.distributed_c10d: [    INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 14:03:47,050 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 0, total 8.
2023-04-10 14:03:47,051 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 4, total 8.
2023-04-10 14:03:47,051 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 5, total 8.
2023-04-10 14:03:47,051 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 2, total 8.
2023-04-10 14:03:47,051 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 6, total 8.
2023-04-10 14:03:47,051 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 3, total 8.
2023-04-10 14:03:47,051 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 7, total 8.
2023-04-10 14:03:47,051 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 1, total 8.
2023-04-10 14:03:47,362 -                train: [    INFO] - Model vit_snn created, param count: 29705768
2023-04-10 14:03:47,362 -          data.config: [    INFO] - Data processing configuration for current model + dataset:
2023-04-10 14:03:47,363 -          data.config: [    INFO] - 	input_size: (3, 224, 224)
2023-04-10 14:03:47,363 -          data.config: [    INFO] - 	interpolation: bicubic
2023-04-10 14:03:47,363 -          data.config: [    INFO] - 	mean: (0.485, 0.456, 0.406)
2023-04-10 14:03:47,363 -          data.config: [    INFO] - 	std: (0.229, 0.224, 0.225)
2023-04-10 14:03:47,363 -          data.config: [    INFO] - 	crop_pct: 1.0
2023-04-10 14:03:52,992 -                train: [    INFO] - Using native Torch AMP. Training in mixed precision.
2023-04-10 14:03:53,200 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 14:03:53,262 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-305.pth.tar' (epoch 305)
2023-04-10 14:03:53,393 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 14:03:53,457 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-308.pth.tar' (epoch 308)
2023-04-10 14:03:53,642 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 14:03:53,708 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-301.pth.tar' (epoch 301)
2023-04-10 14:03:53,896 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 14:03:53,951 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-298.pth.tar' (epoch 298)
2023-04-10 14:03:54,136 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 14:03:54,192 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-290.pth.tar' (epoch 290)
2023-04-10 14:03:54,343 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 14:03:54,407 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-285.pth.tar' (epoch 285)
2023-04-10 14:03:54,552 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 14:03:54,618 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-295.pth.tar' (epoch 295)
2023-04-10 14:03:54,763 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 14:03:54,834 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-300.pth.tar' (epoch 300)
2023-04-10 14:03:54,982 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 14:03:55,042 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-297.pth.tar' (epoch 297)
2023-04-10 14:03:55,184 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 14:03:55,241 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-302.pth.tar' (epoch 302)
2023-04-10 14:03:55,254 -                train: [    INFO] - Converted model to use Synchronized BatchNorm. WARNING: You may have issues if using zero initialized BN layers (enabled by default for ResNets) while sync-bn enabled.
2023-04-10 14:03:55,254 -                train: [    INFO] - Using native Torch DistributedDataParallel.
2023-04-10 14:03:56,596 -                train: [    INFO] - Scheduled epochs: 20
2023-04-10 14:05:18,965 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
2023-04-10 14:05:19,034 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
2023-04-10 14:05:19,098 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
2023-04-10 14:05:19,209 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
2023-04-10 14:05:19,237 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
2023-04-10 14:05:19,273 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
2023-04-10 14:05:19,276 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
2023-04-10 14:05:19,286 - torch.distributed.distributed_c10d: [    INFO] - Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 14:05:19,286 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
2023-04-10 14:05:19,287 - torch.distributed.distributed_c10d: [    INFO] - Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 14:05:19,289 - torch.distributed.distributed_c10d: [    INFO] - Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 14:05:19,292 - torch.distributed.distributed_c10d: [    INFO] - Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 14:05:19,291 - torch.distributed.distributed_c10d: [    INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 14:05:19,292 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 0, total 8.
2023-04-10 14:05:19,293 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 4, total 8.
2023-04-10 14:05:19,293 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 5, total 8.
2023-04-10 14:05:19,293 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 6, total 8.
2023-04-10 14:05:19,293 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 3, total 8.
2023-04-10 14:05:19,294 - torch.distributed.distributed_c10d: [    INFO] - Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 14:05:19,294 - torch.distributed.distributed_c10d: [    INFO] - Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 14:05:19,294 - torch.distributed.distributed_c10d: [    INFO] - Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 14:05:19,294 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 2, total 8.
2023-04-10 14:05:19,295 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 1, total 8.
2023-04-10 14:05:19,295 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 7, total 8.
2023-04-10 14:05:19,591 -                train: [    INFO] - Model vit_snn created, param count: 29705768
2023-04-10 14:05:19,591 -          data.config: [    INFO] - Data processing configuration for current model + dataset:
2023-04-10 14:05:19,591 -          data.config: [    INFO] - 	input_size: (3, 224, 224)
2023-04-10 14:05:19,591 -          data.config: [    INFO] - 	interpolation: bicubic
2023-04-10 14:05:19,591 -          data.config: [    INFO] - 	mean: (0.485, 0.456, 0.406)
2023-04-10 14:05:19,591 -          data.config: [    INFO] - 	std: (0.229, 0.224, 0.225)
2023-04-10 14:05:19,591 -          data.config: [    INFO] - 	crop_pct: 1.0
2023-04-10 14:05:24,585 -                train: [    INFO] - Using native Torch AMP. Training in mixed precision.
2023-04-10 14:05:24,804 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 14:05:24,867 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-305.pth.tar' (epoch 305)
2023-04-10 14:05:25,004 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 14:05:25,056 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-308.pth.tar' (epoch 308)
2023-04-10 14:05:25,192 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 14:05:25,240 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-301.pth.tar' (epoch 301)
2023-04-10 14:05:25,374 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 14:05:25,423 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-298.pth.tar' (epoch 298)
2023-04-10 14:05:25,556 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 14:05:25,608 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-290.pth.tar' (epoch 290)
2023-04-10 14:05:25,740 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 14:05:25,790 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-285.pth.tar' (epoch 285)
2023-04-10 14:05:25,926 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 14:05:25,974 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-295.pth.tar' (epoch 295)
2023-04-10 14:05:26,110 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 14:05:26,157 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-300.pth.tar' (epoch 300)
2023-04-10 14:05:26,293 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 14:05:26,344 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-297.pth.tar' (epoch 297)
2023-04-10 14:05:26,481 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 14:05:26,537 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-302.pth.tar' (epoch 302)
2023-04-10 14:05:26,551 -                train: [    INFO] - Converted model to use Synchronized BatchNorm. WARNING: You may have issues if using zero initialized BN layers (enabled by default for ResNets) while sync-bn enabled.
2023-04-10 14:05:26,551 -                train: [    INFO] - Using native Torch DistributedDataParallel.
2023-04-10 14:05:28,368 -                train: [    INFO] - Scheduled epochs: 20
2023-04-10 19:37:10,992 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
2023-04-10 19:37:11,003 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
2023-04-10 19:37:11,020 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
2023-04-10 19:37:11,038 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
2023-04-10 19:37:11,054 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
2023-04-10 19:37:11,077 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
2023-04-10 19:37:11,086 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
2023-04-10 19:37:11,089 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
2023-04-10 19:37:11,089 - torch.distributed.distributed_c10d: [    INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 19:37:11,089 - torch.distributed.distributed_c10d: [    INFO] - Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 19:37:11,090 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 0, total 8.
2023-04-10 19:37:11,090 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 5, total 8.
2023-04-10 19:37:11,092 - torch.distributed.distributed_c10d: [    INFO] - Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 19:37:11,092 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 1, total 8.
2023-04-10 19:37:11,096 - torch.distributed.distributed_c10d: [    INFO] - Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 19:37:11,096 - torch.distributed.distributed_c10d: [    INFO] - Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 19:37:11,096 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 7, total 8.
2023-04-10 19:37:11,096 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 2, total 8.
2023-04-10 19:37:11,096 - torch.distributed.distributed_c10d: [    INFO] - Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 19:37:11,097 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 6, total 8.
2023-04-10 19:37:11,097 - torch.distributed.distributed_c10d: [    INFO] - Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 19:37:11,097 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 3, total 8.
2023-04-10 19:37:11,098 - torch.distributed.distributed_c10d: [    INFO] - Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 19:37:11,099 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 4, total 8.
2023-04-10 19:37:11,496 -                train: [    INFO] - Model vit_snn created, param count: 29705768
2023-04-10 19:37:11,496 -          data.config: [    INFO] - Data processing configuration for current model + dataset:
2023-04-10 19:37:11,496 -          data.config: [    INFO] - 	input_size: (3, 224, 224)
2023-04-10 19:37:11,496 -          data.config: [    INFO] - 	interpolation: bicubic
2023-04-10 19:37:11,496 -          data.config: [    INFO] - 	mean: (0.485, 0.456, 0.406)
2023-04-10 19:37:11,497 -          data.config: [    INFO] - 	std: (0.229, 0.224, 0.225)
2023-04-10 19:37:11,497 -          data.config: [    INFO] - 	crop_pct: 1.0
2023-04-10 19:37:13,871 -                train: [    INFO] - Using native Torch AMP. Training in mixed precision.
2023-04-10 19:37:14,133 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 19:37:14,188 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-305.pth.tar' (epoch 305)
2023-04-10 19:37:14,486 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 19:37:14,543 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-290.pth.tar' (epoch 290)
2023-04-10 19:37:14,859 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 19:37:14,917 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-298.pth.tar' (epoch 298)
2023-04-10 19:37:15,246 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 19:37:15,302 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-301.pth.tar' (epoch 301)
2023-04-10 19:37:15,624 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 19:37:15,684 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-308.pth.tar' (epoch 308)
2023-04-10 19:37:16,008 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 19:37:16,064 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-285.pth.tar' (epoch 285)
2023-04-10 19:37:16,388 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 19:37:16,453 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-295.pth.tar' (epoch 295)
2023-04-10 19:37:16,814 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 19:37:16,865 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-300.pth.tar' (epoch 300)
2023-04-10 19:37:17,143 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 19:37:17,197 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-297.pth.tar' (epoch 297)
2023-04-10 19:37:17,477 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 19:37:17,529 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-302.pth.tar' (epoch 302)
2023-04-10 19:37:17,577 -                train: [    INFO] - Converted model to use Synchronized BatchNorm. WARNING: You may have issues if using zero initialized BN layers (enabled by default for ResNets) while sync-bn enabled.
2023-04-10 19:37:17,578 -                train: [    INFO] - Using native Torch DistributedDataParallel.
2023-04-10 19:37:18,609 -                train: [   ERROR] - Training folder does not exist at: /root/data/train
2023-04-10 19:37:18,611 -                train: [    INFO] - Scheduled epochs: 20
2023-04-10 19:37:18,611 -                train: [   ERROR] - Training folder does not exist at: /root/data/train
2023-04-10 19:37:18,611 -                train: [   ERROR] - Training folder does not exist at: /root/data/train
2023-04-10 19:37:18,611 -                train: [   ERROR] - Training folder does not exist at: /root/data/train
2023-04-10 19:37:18,611 -                train: [   ERROR] - Training folder does not exist at: /root/data/train
2023-04-10 19:37:18,611 -                train: [   ERROR] - Training folder does not exist at: /root/data/train
2023-04-10 19:37:18,611 -                train: [   ERROR] - Training folder does not exist at: /root/data/train
2023-04-10 19:37:18,612 -                train: [   ERROR] - Training folder does not exist at: /root/data/train
2023-04-10 19:38:44,045 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
2023-04-10 19:38:44,046 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
2023-04-10 19:38:44,061 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
2023-04-10 19:38:44,078 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
2023-04-10 19:38:44,108 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
2023-04-10 19:38:44,116 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
2023-04-10 19:38:44,118 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
2023-04-10 19:38:44,119 - torch.distributed.distributed_c10d: [    INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 19:38:44,119 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
2023-04-10 19:38:44,120 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 0, total 8.
2023-04-10 19:38:44,120 - torch.distributed.distributed_c10d: [    INFO] - Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 19:38:44,120 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 2, total 8.
2023-04-10 19:38:44,123 - torch.distributed.distributed_c10d: [    INFO] - Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 19:38:44,124 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 6, total 8.
2023-04-10 19:38:44,127 - torch.distributed.distributed_c10d: [    INFO] - Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 19:38:44,127 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 7, total 8.
2023-04-10 19:38:44,129 - torch.distributed.distributed_c10d: [    INFO] - Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 19:38:44,129 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 3, total 8.
2023-04-10 19:38:44,129 - torch.distributed.distributed_c10d: [    INFO] - Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 19:38:44,129 - torch.distributed.distributed_c10d: [    INFO] - Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 19:38:44,129 - torch.distributed.distributed_c10d: [    INFO] - Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 19:38:44,130 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 5, total 8.
2023-04-10 19:38:44,130 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 4, total 8.
2023-04-10 19:38:44,130 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 1, total 8.
2023-04-10 19:38:44,525 -                train: [    INFO] - Model vit_snn created, param count: 29705768
2023-04-10 19:38:44,525 -          data.config: [    INFO] - Data processing configuration for current model + dataset:
2023-04-10 19:38:44,526 -          data.config: [    INFO] - 	input_size: (3, 224, 224)
2023-04-10 19:38:44,526 -          data.config: [    INFO] - 	interpolation: bicubic
2023-04-10 19:38:44,526 -          data.config: [    INFO] - 	mean: (0.485, 0.456, 0.406)
2023-04-10 19:38:44,526 -          data.config: [    INFO] - 	std: (0.229, 0.224, 0.225)
2023-04-10 19:38:44,526 -          data.config: [    INFO] - 	crop_pct: 1.0
2023-04-10 19:38:46,879 -                train: [    INFO] - Using native Torch AMP. Training in mixed precision.
2023-04-10 19:38:47,147 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 19:38:47,200 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-305.pth.tar' (epoch 305)
2023-04-10 19:38:47,519 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 19:38:47,573 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-290.pth.tar' (epoch 290)
2023-04-10 19:38:47,729 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 19:38:47,784 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-298.pth.tar' (epoch 298)
2023-04-10 19:38:47,939 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 19:38:47,993 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-301.pth.tar' (epoch 301)
2023-04-10 19:38:48,148 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 19:38:48,202 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-308.pth.tar' (epoch 308)
2023-04-10 19:38:48,362 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 19:38:48,417 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-285.pth.tar' (epoch 285)
2023-04-10 19:38:48,576 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 19:38:48,629 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-295.pth.tar' (epoch 295)
2023-04-10 19:38:48,885 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 19:38:48,938 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-300.pth.tar' (epoch 300)
2023-04-10 19:38:49,097 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 19:38:49,150 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-297.pth.tar' (epoch 297)
2023-04-10 19:38:49,308 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 19:38:49,377 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-302.pth.tar' (epoch 302)
2023-04-10 19:38:49,396 -                train: [    INFO] - Converted model to use Synchronized BatchNorm. WARNING: You may have issues if using zero initialized BN layers (enabled by default for ResNets) while sync-bn enabled.
2023-04-10 19:38:49,396 -                train: [    INFO] - Using native Torch DistributedDataParallel.
2023-04-10 19:38:50,739 -                train: [    INFO] - Scheduled epochs: 20
2023-04-10 20:16:12,511 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
2023-04-10 20:16:12,524 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
2023-04-10 20:16:12,538 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
2023-04-10 20:16:12,557 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
2023-04-10 20:16:12,593 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
2023-04-10 20:16:12,593 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
2023-04-10 20:16:12,599 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
2023-04-10 20:16:12,599 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
2023-04-10 20:16:12,599 - torch.distributed.distributed_c10d: [    INFO] - Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 20:16:12,599 - torch.distributed.distributed_c10d: [    INFO] - Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 20:16:12,599 - torch.distributed.distributed_c10d: [    INFO] - Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 20:16:12,600 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 5, total 8.
2023-04-10 20:16:12,600 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 6, total 8.
2023-04-10 20:16:12,600 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 7, total 8.
2023-04-10 20:16:12,600 - torch.distributed.distributed_c10d: [    INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 20:16:12,601 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 0, total 8.
2023-04-10 20:16:12,603 - torch.distributed.distributed_c10d: [    INFO] - Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 20:16:12,604 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 2, total 8.
2023-04-10 20:16:12,604 - torch.distributed.distributed_c10d: [    INFO] - Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 20:16:12,604 - torch.distributed.distributed_c10d: [    INFO] - Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 20:16:12,604 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 3, total 8.
2023-04-10 20:16:12,604 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 4, total 8.
2023-04-10 20:16:12,607 - torch.distributed.distributed_c10d: [    INFO] - Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 20:16:12,607 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 1, total 8.
2023-04-10 20:16:13,003 -                train: [    INFO] - Model vit_snn created, param count: 29705768
2023-04-10 20:16:13,003 -          data.config: [    INFO] - Data processing configuration for current model + dataset:
2023-04-10 20:16:13,003 -          data.config: [    INFO] - 	input_size: (3, 224, 224)
2023-04-10 20:16:13,003 -          data.config: [    INFO] - 	interpolation: bicubic
2023-04-10 20:16:13,004 -          data.config: [    INFO] - 	mean: (0.485, 0.456, 0.406)
2023-04-10 20:16:13,004 -          data.config: [    INFO] - 	std: (0.229, 0.224, 0.225)
2023-04-10 20:16:13,004 -          data.config: [    INFO] - 	crop_pct: 1.0
2023-04-10 20:16:15,323 -                train: [    INFO] - Using native Torch AMP. Training in mixed precision.
2023-04-10 20:16:15,597 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 20:16:15,650 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-305.pth.tar' (epoch 305)
2023-04-10 20:16:15,935 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 20:16:15,986 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-290.pth.tar' (epoch 290)
2023-04-10 20:16:16,152 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 20:16:16,205 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-298.pth.tar' (epoch 298)
2023-04-10 20:16:16,382 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 20:16:16,435 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-301.pth.tar' (epoch 301)
2023-04-10 20:16:16,614 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 20:16:16,671 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-308.pth.tar' (epoch 308)
2023-04-10 20:16:16,854 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 20:16:16,907 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-285.pth.tar' (epoch 285)
2023-04-10 20:16:17,091 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 20:16:17,144 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-295.pth.tar' (epoch 295)
2023-04-10 20:16:17,412 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 20:16:17,465 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-300.pth.tar' (epoch 300)
2023-04-10 20:16:17,648 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 20:16:17,701 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-297.pth.tar' (epoch 297)
2023-04-10 20:16:17,899 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 20:16:17,954 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-302.pth.tar' (epoch 302)
2023-04-10 20:16:17,973 -                train: [    INFO] - Converted model to use Synchronized BatchNorm. WARNING: You may have issues if using zero initialized BN layers (enabled by default for ResNets) while sync-bn enabled.
2023-04-10 20:16:17,973 -                train: [    INFO] - Using native Torch DistributedDataParallel.
2023-04-10 20:16:20,696 -                train: [    INFO] - Scheduled epochs: 20
2023-04-10 21:04:49,930 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
2023-04-10 21:04:49,941 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
2023-04-10 21:04:49,960 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
2023-04-10 21:04:49,968 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
2023-04-10 21:04:50,002 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
2023-04-10 21:04:50,004 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
2023-04-10 21:04:50,012 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
2023-04-10 21:04:50,014 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
2023-04-10 21:04:50,014 - torch.distributed.distributed_c10d: [    INFO] - Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:04:50,015 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 6, total 8.
2023-04-10 21:04:50,015 - torch.distributed.distributed_c10d: [    INFO] - Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:04:50,015 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 1, total 8.
2023-04-10 21:04:50,020 - torch.distributed.distributed_c10d: [    INFO] - Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:04:50,020 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 2, total 8.
2023-04-10 21:04:50,022 - torch.distributed.distributed_c10d: [    INFO] - Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:04:50,022 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 3, total 8.
2023-04-10 21:04:50,023 - torch.distributed.distributed_c10d: [    INFO] - Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:04:50,023 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 4, total 8.
2023-04-10 21:04:50,023 - torch.distributed.distributed_c10d: [    INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:04:50,023 - torch.distributed.distributed_c10d: [    INFO] - Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:04:50,023 - torch.distributed.distributed_c10d: [    INFO] - Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:04:50,023 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 0, total 8.
2023-04-10 21:04:50,024 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 5, total 8.
2023-04-10 21:04:50,024 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 7, total 8.
2023-04-10 21:04:50,401 -                train: [    INFO] - Model vit_snn created, param count: 29705768
2023-04-10 21:04:50,402 -          data.config: [    INFO] - Data processing configuration for current model + dataset:
2023-04-10 21:04:50,402 -          data.config: [    INFO] - 	input_size: (3, 224, 224)
2023-04-10 21:04:50,402 -          data.config: [    INFO] - 	interpolation: bicubic
2023-04-10 21:04:50,402 -          data.config: [    INFO] - 	mean: (0.485, 0.456, 0.406)
2023-04-10 21:04:50,402 -          data.config: [    INFO] - 	std: (0.229, 0.224, 0.225)
2023-04-10 21:04:50,402 -          data.config: [    INFO] - 	crop_pct: 1.0
2023-04-10 21:04:52,791 -                train: [    INFO] - Using native Torch AMP. Training in mixed precision.
2023-04-10 21:04:53,058 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:04:53,110 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-305.pth.tar' (epoch 305)
2023-04-10 21:04:53,417 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:04:53,471 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-290.pth.tar' (epoch 290)
2023-04-10 21:04:53,624 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:04:53,677 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-298.pth.tar' (epoch 298)
2023-04-10 21:04:53,830 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:04:53,881 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-301.pth.tar' (epoch 301)
2023-04-10 21:04:54,037 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:04:54,089 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-308.pth.tar' (epoch 308)
2023-04-10 21:04:54,246 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:04:54,298 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-285.pth.tar' (epoch 285)
2023-04-10 21:04:54,454 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:04:54,506 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-295.pth.tar' (epoch 295)
2023-04-10 21:04:54,737 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:04:54,789 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-300.pth.tar' (epoch 300)
2023-04-10 21:04:54,946 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:04:54,999 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-297.pth.tar' (epoch 297)
2023-04-10 21:04:55,167 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:04:55,246 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-302.pth.tar' (epoch 302)
2023-04-10 21:04:55,265 -                train: [    INFO] - Converted model to use Synchronized BatchNorm. WARNING: You may have issues if using zero initialized BN layers (enabled by default for ResNets) while sync-bn enabled.
2023-04-10 21:04:55,266 -                train: [    INFO] - Using native Torch DistributedDataParallel.
2023-04-10 21:04:57,995 -                train: [    INFO] - Scheduled epochs: 20
2023-04-10 21:09:55,450 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
2023-04-10 21:09:55,458 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
2023-04-10 21:09:55,478 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
2023-04-10 21:09:55,490 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
2023-04-10 21:09:55,532 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
2023-04-10 21:09:55,533 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
2023-04-10 21:09:55,535 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
2023-04-10 21:09:55,537 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
2023-04-10 21:09:55,537 - torch.distributed.distributed_c10d: [    INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:09:55,537 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 0, total 8.
2023-04-10 21:09:55,540 - torch.distributed.distributed_c10d: [    INFO] - Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:09:55,541 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 7, total 8.
2023-04-10 21:09:55,541 - torch.distributed.distributed_c10d: [    INFO] - Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:09:55,541 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 5, total 8.
2023-04-10 21:09:55,542 - torch.distributed.distributed_c10d: [    INFO] - Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:09:55,543 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 2, total 8.
2023-04-10 21:09:55,543 - torch.distributed.distributed_c10d: [    INFO] - Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:09:55,543 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 6, total 8.
2023-04-10 21:09:55,543 - torch.distributed.distributed_c10d: [    INFO] - Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:09:55,544 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 3, total 8.
2023-04-10 21:09:55,544 - torch.distributed.distributed_c10d: [    INFO] - Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:09:55,544 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 4, total 8.
2023-04-10 21:09:55,547 - torch.distributed.distributed_c10d: [    INFO] - Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:09:55,547 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 1, total 8.
2023-04-10 21:09:55,936 -                train: [    INFO] - Model vit_snn created, param count: 29705768
2023-04-10 21:09:55,936 -          data.config: [    INFO] - Data processing configuration for current model + dataset:
2023-04-10 21:09:55,937 -          data.config: [    INFO] - 	input_size: (3, 224, 224)
2023-04-10 21:09:55,937 -          data.config: [    INFO] - 	interpolation: bicubic
2023-04-10 21:09:55,937 -          data.config: [    INFO] - 	mean: (0.485, 0.456, 0.406)
2023-04-10 21:09:55,937 -          data.config: [    INFO] - 	std: (0.229, 0.224, 0.225)
2023-04-10 21:09:55,937 -          data.config: [    INFO] - 	crop_pct: 1.0
2023-04-10 21:09:58,416 -                train: [    INFO] - Using native Torch AMP. Training in mixed precision.
2023-04-10 21:09:58,720 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:09:58,786 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-305.pth.tar' (epoch 305)
2023-04-10 21:09:59,112 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:09:59,174 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-290.pth.tar' (epoch 290)
2023-04-10 21:09:59,335 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:09:59,397 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-298.pth.tar' (epoch 298)
2023-04-10 21:09:59,560 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:09:59,624 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-301.pth.tar' (epoch 301)
2023-04-10 21:09:59,789 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:09:59,852 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-308.pth.tar' (epoch 308)
2023-04-10 21:10:00,018 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:10:00,082 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-285.pth.tar' (epoch 285)
2023-04-10 21:10:00,246 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:10:00,309 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-295.pth.tar' (epoch 295)
2023-04-10 21:10:00,572 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:10:00,635 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-300.pth.tar' (epoch 300)
2023-04-10 21:10:00,802 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:10:00,894 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-297.pth.tar' (epoch 297)
2023-04-10 21:10:01,088 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:10:01,159 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-302.pth.tar' (epoch 302)
2023-04-10 21:10:01,186 -                train: [    INFO] - Converted model to use Synchronized BatchNorm. WARNING: You may have issues if using zero initialized BN layers (enabled by default for ResNets) while sync-bn enabled.
2023-04-10 21:10:01,187 -                train: [    INFO] - Using native Torch DistributedDataParallel.
2023-04-10 21:10:03,461 -                train: [    INFO] - Scheduled epochs: 20
2023-04-10 21:11:26,239 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
2023-04-10 21:11:26,263 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
2023-04-10 21:11:26,304 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
2023-04-10 21:11:26,305 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
2023-04-10 21:11:26,308 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
2023-04-10 21:11:26,334 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
2023-04-10 21:11:26,336 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
2023-04-10 21:11:26,336 - torch.distributed.distributed_c10d: [    INFO] - Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:11:26,336 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
2023-04-10 21:11:26,337 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 1, total 8.
2023-04-10 21:11:26,337 - torch.distributed.distributed_c10d: [    INFO] - Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:11:26,337 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 3, total 8.
2023-04-10 21:11:26,339 - torch.distributed.distributed_c10d: [    INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:11:26,339 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 0, total 8.
2023-04-10 21:11:26,343 - torch.distributed.distributed_c10d: [    INFO] - Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:11:26,343 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 4, total 8.
2023-04-10 21:11:26,345 - torch.distributed.distributed_c10d: [    INFO] - Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:11:26,346 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 2, total 8.
2023-04-10 21:11:26,346 - torch.distributed.distributed_c10d: [    INFO] - Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:11:26,346 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 5, total 8.
2023-04-10 21:11:26,347 - torch.distributed.distributed_c10d: [    INFO] - Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:11:26,347 - torch.distributed.distributed_c10d: [    INFO] - Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:11:26,347 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 6, total 8.
2023-04-10 21:11:26,347 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 7, total 8.
2023-04-10 21:11:26,740 -                train: [    INFO] - Model vit_snn created, param count: 29705768
2023-04-10 21:11:26,740 -          data.config: [    INFO] - Data processing configuration for current model + dataset:
2023-04-10 21:11:26,740 -          data.config: [    INFO] - 	input_size: (3, 224, 224)
2023-04-10 21:11:26,740 -          data.config: [    INFO] - 	interpolation: bicubic
2023-04-10 21:11:26,741 -          data.config: [    INFO] - 	mean: (0.485, 0.456, 0.406)
2023-04-10 21:11:26,741 -          data.config: [    INFO] - 	std: (0.229, 0.224, 0.225)
2023-04-10 21:11:26,741 -          data.config: [    INFO] - 	crop_pct: 1.0
2023-04-10 21:11:29,063 -                train: [    INFO] - Using native Torch AMP. Training in mixed precision.
2023-04-10 21:11:29,347 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:11:29,400 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-305.pth.tar' (epoch 305)
2023-04-10 21:11:29,737 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:11:29,789 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-290.pth.tar' (epoch 290)
2023-04-10 21:11:29,962 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:11:30,015 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-298.pth.tar' (epoch 298)
2023-04-10 21:11:30,191 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:11:30,244 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-301.pth.tar' (epoch 301)
2023-04-10 21:11:30,422 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:11:30,476 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-308.pth.tar' (epoch 308)
2023-04-10 21:11:30,666 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:11:30,718 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-285.pth.tar' (epoch 285)
2023-04-10 21:11:30,908 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:11:30,985 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-295.pth.tar' (epoch 295)
2023-04-10 21:11:31,271 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:11:31,331 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-300.pth.tar' (epoch 300)
2023-04-10 21:11:31,554 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:11:31,608 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-297.pth.tar' (epoch 297)
2023-04-10 21:11:31,825 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:11:31,881 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-302.pth.tar' (epoch 302)
2023-04-10 21:11:31,912 -                train: [    INFO] - Converted model to use Synchronized BatchNorm. WARNING: You may have issues if using zero initialized BN layers (enabled by default for ResNets) while sync-bn enabled.
2023-04-10 21:11:31,912 -                train: [    INFO] - Using native Torch DistributedDataParallel.
2023-04-10 21:11:33,439 -                train: [    INFO] - Scheduled epochs: 20
2023-04-10 21:14:43,268 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
2023-04-10 21:14:43,275 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
2023-04-10 21:14:43,299 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
2023-04-10 21:14:43,314 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
2023-04-10 21:14:43,354 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
2023-04-10 21:14:43,362 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
2023-04-10 21:14:43,367 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
2023-04-10 21:14:43,369 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
2023-04-10 21:14:43,369 - torch.distributed.distributed_c10d: [    INFO] - Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:14:43,369 - torch.distributed.distributed_c10d: [    INFO] - Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:14:43,369 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 5, total 8.
2023-04-10 21:14:43,370 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 3, total 8.
2023-04-10 21:14:43,372 - torch.distributed.distributed_c10d: [    INFO] - Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:14:43,372 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 1, total 8.
2023-04-10 21:14:43,373 - torch.distributed.distributed_c10d: [    INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:14:43,373 - torch.distributed.distributed_c10d: [    INFO] - Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:14:43,373 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 0, total 8.
2023-04-10 21:14:43,373 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 6, total 8.
2023-04-10 21:14:43,375 - torch.distributed.distributed_c10d: [    INFO] - Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:14:43,376 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 7, total 8.
2023-04-10 21:14:43,376 - torch.distributed.distributed_c10d: [    INFO] - Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:14:43,377 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 4, total 8.
2023-04-10 21:14:43,378 - torch.distributed.distributed_c10d: [    INFO] - Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:14:43,379 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 2, total 8.
2023-04-10 21:14:43,778 -                train: [    INFO] - Model vit_snn created, param count: 29705768
2023-04-10 21:14:43,778 -          data.config: [    INFO] - Data processing configuration for current model + dataset:
2023-04-10 21:14:43,778 -          data.config: [    INFO] - 	input_size: (3, 224, 224)
2023-04-10 21:14:43,778 -          data.config: [    INFO] - 	interpolation: bicubic
2023-04-10 21:14:43,778 -          data.config: [    INFO] - 	mean: (0.485, 0.456, 0.406)
2023-04-10 21:14:43,779 -          data.config: [    INFO] - 	std: (0.229, 0.224, 0.225)
2023-04-10 21:14:43,779 -          data.config: [    INFO] - 	crop_pct: 1.0
2023-04-10 21:14:46,105 -                train: [    INFO] - Using native Torch AMP. Training in mixed precision.
2023-04-10 21:14:46,373 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:14:46,426 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-305.pth.tar' (epoch 305)
2023-04-10 21:14:46,741 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:14:46,793 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-290.pth.tar' (epoch 290)
2023-04-10 21:14:46,946 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:14:46,998 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-298.pth.tar' (epoch 298)
2023-04-10 21:14:47,151 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:14:47,203 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-301.pth.tar' (epoch 301)
2023-04-10 21:14:47,358 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:14:47,409 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-308.pth.tar' (epoch 308)
2023-04-10 21:14:47,569 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:14:47,622 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-285.pth.tar' (epoch 285)
2023-04-10 21:14:47,780 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:14:47,833 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-295.pth.tar' (epoch 295)
2023-04-10 21:14:48,087 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:14:48,138 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-300.pth.tar' (epoch 300)
2023-04-10 21:14:48,296 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:14:48,349 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-297.pth.tar' (epoch 297)
2023-04-10 21:14:48,507 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:14:48,565 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-302.pth.tar' (epoch 302)
2023-04-10 21:14:48,583 -                train: [    INFO] - Converted model to use Synchronized BatchNorm. WARNING: You may have issues if using zero initialized BN layers (enabled by default for ResNets) while sync-bn enabled.
2023-04-10 21:14:48,584 -                train: [    INFO] - Using native Torch DistributedDataParallel.
2023-04-10 21:14:51,073 -                train: [    INFO] - Scheduled epochs: 20
2023-04-10 21:17:11,850 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
2023-04-10 21:17:11,867 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
2023-04-10 21:17:11,880 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
2023-04-10 21:17:11,901 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
2023-04-10 21:17:11,921 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
2023-04-10 21:17:11,928 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
2023-04-10 21:17:11,937 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
2023-04-10 21:17:11,940 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
2023-04-10 21:17:11,941 - torch.distributed.distributed_c10d: [    INFO] - Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:17:11,941 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 2, total 8.
2023-04-10 21:17:11,942 - torch.distributed.distributed_c10d: [    INFO] - Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:17:11,942 - torch.distributed.distributed_c10d: [    INFO] - Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:17:11,942 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 1, total 8.
2023-04-10 21:17:11,942 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 7, total 8.
2023-04-10 21:17:11,943 - torch.distributed.distributed_c10d: [    INFO] - Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:17:11,943 - torch.distributed.distributed_c10d: [    INFO] - Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:17:11,943 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 4, total 8.
2023-04-10 21:17:11,943 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 5, total 8.
2023-04-10 21:17:11,948 - torch.distributed.distributed_c10d: [    INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:17:11,948 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 0, total 8.
2023-04-10 21:17:11,949 - torch.distributed.distributed_c10d: [    INFO] - Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:17:11,949 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 6, total 8.
2023-04-10 21:17:11,950 - torch.distributed.distributed_c10d: [    INFO] - Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:17:11,950 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 3, total 8.
2023-04-10 21:17:12,368 -                train: [    INFO] - Model vit_snn created, param count: 29705768
2023-04-10 21:17:12,368 -          data.config: [    INFO] - Data processing configuration for current model + dataset:
2023-04-10 21:17:12,368 -          data.config: [    INFO] - 	input_size: (3, 224, 224)
2023-04-10 21:17:12,368 -          data.config: [    INFO] - 	interpolation: bicubic
2023-04-10 21:17:12,369 -          data.config: [    INFO] - 	mean: (0.485, 0.456, 0.406)
2023-04-10 21:17:12,369 -          data.config: [    INFO] - 	std: (0.229, 0.224, 0.225)
2023-04-10 21:17:12,369 -          data.config: [    INFO] - 	crop_pct: 1.0
2023-04-10 21:17:14,719 -                train: [    INFO] - Using native Torch AMP. Training in mixed precision.
2023-04-10 21:17:14,988 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:17:15,041 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-305.pth.tar' (epoch 305)
2023-04-10 21:17:15,350 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:17:15,404 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-290.pth.tar' (epoch 290)
2023-04-10 21:17:15,721 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:17:15,775 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-298.pth.tar' (epoch 298)
2023-04-10 21:17:16,091 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:17:16,145 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-301.pth.tar' (epoch 301)
2023-04-10 21:17:16,459 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:17:16,512 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-308.pth.tar' (epoch 308)
2023-04-10 21:17:16,824 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:17:16,878 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-285.pth.tar' (epoch 285)
2023-04-10 21:17:17,196 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:17:17,280 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-295.pth.tar' (epoch 295)
2023-04-10 21:17:17,640 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:17:17,693 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-300.pth.tar' (epoch 300)
2023-04-10 21:17:17,994 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:17:18,047 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-297.pth.tar' (epoch 297)
2023-04-10 21:17:18,348 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:17:18,404 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-302.pth.tar' (epoch 302)
2023-04-10 21:17:18,454 -                train: [    INFO] - Converted model to use Synchronized BatchNorm. WARNING: You may have issues if using zero initialized BN layers (enabled by default for ResNets) while sync-bn enabled.
2023-04-10 21:17:18,455 -                train: [    INFO] - Using native Torch DistributedDataParallel.
2023-04-10 21:17:19,557 -                train: [    INFO] - Scheduled epochs: 20
2023-04-10 21:23:02,070 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
2023-04-10 21:23:02,091 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
2023-04-10 21:23:02,106 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
2023-04-10 21:23:02,140 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
2023-04-10 21:23:02,146 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
2023-04-10 21:23:02,172 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
2023-04-10 21:23:02,178 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
2023-04-10 21:23:02,178 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
2023-04-10 21:23:02,179 - torch.distributed.distributed_c10d: [    INFO] - Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:23:02,179 - torch.distributed.distributed_c10d: [    INFO] - Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:23:02,179 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 5, total 8.
2023-04-10 21:23:02,179 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 7, total 8.
2023-04-10 21:23:02,182 - torch.distributed.distributed_c10d: [    INFO] - Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:23:02,183 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 1, total 8.
2023-04-10 21:23:02,183 - torch.distributed.distributed_c10d: [    INFO] - Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:23:02,183 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 2, total 8.
2023-04-10 21:23:02,184 - torch.distributed.distributed_c10d: [    INFO] - Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:23:02,184 - torch.distributed.distributed_c10d: [    INFO] - Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:23:02,185 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 4, total 8.
2023-04-10 21:23:02,185 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 3, total 8.
2023-04-10 21:23:02,188 - torch.distributed.distributed_c10d: [    INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:23:02,188 - torch.distributed.distributed_c10d: [    INFO] - Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:23:02,188 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 0, total 8.
2023-04-10 21:23:02,189 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 6, total 8.
2023-04-10 21:23:02,533 -                train: [    INFO] - Model vit_snn created, param count: 29705768
2023-04-10 21:23:02,533 -          data.config: [    INFO] - Data processing configuration for current model + dataset:
2023-04-10 21:23:02,533 -          data.config: [    INFO] - 	input_size: (3, 224, 224)
2023-04-10 21:23:02,533 -          data.config: [    INFO] - 	interpolation: bicubic
2023-04-10 21:23:02,533 -          data.config: [    INFO] - 	mean: (0.485, 0.456, 0.406)
2023-04-10 21:23:02,533 -          data.config: [    INFO] - 	std: (0.229, 0.224, 0.225)
2023-04-10 21:23:02,534 -          data.config: [    INFO] - 	crop_pct: 1.0
2023-04-10 21:23:05,072 -                train: [    INFO] - Using native Torch AMP. Training in mixed precision.
2023-04-10 21:23:05,346 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:23:05,406 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-305.pth.tar' (epoch 305)
2023-04-10 21:23:05,725 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:23:05,787 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-290.pth.tar' (epoch 290)
2023-04-10 21:23:05,942 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:23:06,002 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-298.pth.tar' (epoch 298)
2023-04-10 21:23:06,158 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:23:06,218 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-301.pth.tar' (epoch 301)
2023-04-10 21:23:06,376 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:23:06,436 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-308.pth.tar' (epoch 308)
2023-04-10 21:23:06,596 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:23:06,659 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-285.pth.tar' (epoch 285)
2023-04-10 21:23:06,818 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:23:06,880 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-295.pth.tar' (epoch 295)
2023-04-10 21:23:07,141 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:23:07,203 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-300.pth.tar' (epoch 300)
2023-04-10 21:23:07,364 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:23:07,448 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-297.pth.tar' (epoch 297)
2023-04-10 21:23:07,612 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:23:07,669 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-302.pth.tar' (epoch 302)
2023-04-10 21:23:07,688 -                train: [    INFO] - Converted model to use Synchronized BatchNorm. WARNING: You may have issues if using zero initialized BN layers (enabled by default for ResNets) while sync-bn enabled.
2023-04-10 21:23:07,688 -                train: [    INFO] - Using native Torch DistributedDataParallel.
2023-04-10 21:23:10,109 -                train: [    INFO] - Scheduled epochs: 20
2023-04-10 21:24:25,771 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
2023-04-10 21:24:25,786 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
2023-04-10 21:24:25,797 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
2023-04-10 21:24:25,823 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
2023-04-10 21:24:25,845 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
2023-04-10 21:24:25,863 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
2023-04-10 21:24:25,863 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
2023-04-10 21:24:25,865 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
2023-04-10 21:24:25,866 - torch.distributed.distributed_c10d: [    INFO] - Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:24:25,866 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 6, total 8.
2023-04-10 21:24:25,866 - torch.distributed.distributed_c10d: [    INFO] - Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:24:25,867 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 5, total 8.
2023-04-10 21:24:25,869 - torch.distributed.distributed_c10d: [    INFO] - Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:24:25,869 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 1, total 8.
2023-04-10 21:24:25,870 - torch.distributed.distributed_c10d: [    INFO] - Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:24:25,871 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 4, total 8.
2023-04-10 21:24:25,873 - torch.distributed.distributed_c10d: [    INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:24:25,874 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 0, total 8.
2023-04-10 21:24:25,874 - torch.distributed.distributed_c10d: [    INFO] - Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:24:25,875 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 7, total 8.
2023-04-10 21:24:25,875 - torch.distributed.distributed_c10d: [    INFO] - Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:24:25,875 - torch.distributed.distributed_c10d: [    INFO] - Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:24:25,876 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 2, total 8.
2023-04-10 21:24:25,876 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 3, total 8.
2023-04-10 21:24:26,256 -                train: [    INFO] - Model vit_snn created, param count: 29705768
2023-04-10 21:24:26,256 -          data.config: [    INFO] - Data processing configuration for current model + dataset:
2023-04-10 21:24:26,256 -          data.config: [    INFO] - 	input_size: (3, 224, 224)
2023-04-10 21:24:26,256 -          data.config: [    INFO] - 	interpolation: bicubic
2023-04-10 21:24:26,257 -          data.config: [    INFO] - 	mean: (0.485, 0.456, 0.406)
2023-04-10 21:24:26,257 -          data.config: [    INFO] - 	std: (0.229, 0.224, 0.225)
2023-04-10 21:24:26,257 -          data.config: [    INFO] - 	crop_pct: 1.0
2023-04-10 21:24:28,701 -                train: [    INFO] - Using native Torch AMP. Training in mixed precision.
2023-04-10 21:24:28,983 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:24:29,034 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-305.pth.tar' (epoch 305)
2023-04-10 21:24:29,350 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:24:29,400 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-290.pth.tar' (epoch 290)
2023-04-10 21:24:29,737 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:24:29,788 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-298.pth.tar' (epoch 298)
2023-04-10 21:24:30,135 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:24:30,189 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-301.pth.tar' (epoch 301)
2023-04-10 21:24:30,529 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:24:30,581 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-308.pth.tar' (epoch 308)
2023-04-10 21:24:30,927 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:24:30,979 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-285.pth.tar' (epoch 285)
2023-04-10 21:24:31,346 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:24:31,410 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-295.pth.tar' (epoch 295)
2023-04-10 21:24:31,796 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:24:31,846 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-300.pth.tar' (epoch 300)
2023-04-10 21:24:32,161 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:24:32,211 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-297.pth.tar' (epoch 297)
2023-04-10 21:24:32,499 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:24:32,546 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-302.pth.tar' (epoch 302)
2023-04-10 21:24:32,587 -                train: [    INFO] - Converted model to use Synchronized BatchNorm. WARNING: You may have issues if using zero initialized BN layers (enabled by default for ResNets) while sync-bn enabled.
2023-04-10 21:24:32,587 -                train: [    INFO] - Using native Torch DistributedDataParallel.
2023-04-10 21:24:33,550 -                train: [    INFO] - Scheduled epochs: 20
2023-04-10 21:25:39,237 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
2023-04-10 21:25:39,251 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
2023-04-10 21:25:39,265 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
2023-04-10 21:25:39,278 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
2023-04-10 21:25:39,295 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
2023-04-10 21:25:39,310 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
2023-04-10 21:25:39,325 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
2023-04-10 21:25:39,326 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
2023-04-10 21:25:39,327 - torch.distributed.distributed_c10d: [    INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:25:39,326 - torch.distributed.distributed_c10d: [    INFO] - Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:25:39,327 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 0, total 8.
2023-04-10 21:25:39,327 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 3, total 8.
2023-04-10 21:25:39,327 - torch.distributed.distributed_c10d: [    INFO] - Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:25:39,327 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 7, total 8.
2023-04-10 21:25:39,330 - torch.distributed.distributed_c10d: [    INFO] - Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:25:39,330 - torch.distributed.distributed_c10d: [    INFO] - Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:25:39,330 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 1, total 8.
2023-04-10 21:25:39,330 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 5, total 8.
2023-04-10 21:25:39,331 - torch.distributed.distributed_c10d: [    INFO] - Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:25:39,331 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 2, total 8.
2023-04-10 21:25:39,334 - torch.distributed.distributed_c10d: [    INFO] - Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:25:39,335 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 4, total 8.
2023-04-10 21:25:39,336 - torch.distributed.distributed_c10d: [    INFO] - Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:25:39,336 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 6, total 8.
2023-04-10 21:25:39,711 -                train: [    INFO] - Model vit_snn created, param count: 29705768
2023-04-10 21:25:39,711 -          data.config: [    INFO] - Data processing configuration for current model + dataset:
2023-04-10 21:25:39,711 -          data.config: [    INFO] - 	input_size: (3, 224, 224)
2023-04-10 21:25:39,712 -          data.config: [    INFO] - 	interpolation: bicubic
2023-04-10 21:25:39,712 -          data.config: [    INFO] - 	mean: (0.485, 0.456, 0.406)
2023-04-10 21:25:39,712 -          data.config: [    INFO] - 	std: (0.229, 0.224, 0.225)
2023-04-10 21:25:39,712 -          data.config: [    INFO] - 	crop_pct: 1.0
2023-04-10 21:25:42,290 -                train: [    INFO] - Using native Torch AMP. Training in mixed precision.
2023-04-10 21:25:42,584 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:25:42,642 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-305.pth.tar' (epoch 305)
2023-04-10 21:25:42,968 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:25:43,025 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-290.pth.tar' (epoch 290)
2023-04-10 21:25:43,350 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:25:43,405 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-298.pth.tar' (epoch 298)
2023-04-10 21:25:43,770 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:25:43,831 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-301.pth.tar' (epoch 301)
2023-04-10 21:25:44,165 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:25:44,221 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-308.pth.tar' (epoch 308)
2023-04-10 21:25:44,532 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:25:44,587 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-285.pth.tar' (epoch 285)
2023-04-10 21:25:44,904 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:25:44,954 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-295.pth.tar' (epoch 295)
2023-04-10 21:25:45,358 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:25:45,434 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-300.pth.tar' (epoch 300)
2023-04-10 21:25:45,797 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:25:45,857 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-297.pth.tar' (epoch 297)
2023-04-10 21:25:46,167 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:25:46,224 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-302.pth.tar' (epoch 302)
2023-04-10 21:25:46,278 -                train: [    INFO] - Converted model to use Synchronized BatchNorm. WARNING: You may have issues if using zero initialized BN layers (enabled by default for ResNets) while sync-bn enabled.
2023-04-10 21:25:46,278 -                train: [    INFO] - Using native Torch DistributedDataParallel.
2023-04-10 21:25:47,253 -                train: [    INFO] - Scheduled epochs: 20
2023-04-10 21:26:12,194 -                train: [    INFO] - Test: [   0/65]  Time: 5.543 (5.543)  DataTime: 2.953 (2.953)  Loss:  0.8027 (0.8027)  Acc@1: 85.8073 (85.8073)  Acc@5: 97.0052 (97.0052)
2023-04-10 21:26:49,794 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
2023-04-10 21:26:49,817 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
2023-04-10 21:26:49,836 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
2023-04-10 21:26:49,851 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
2023-04-10 21:26:49,880 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
2023-04-10 21:26:49,899 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
2023-04-10 21:26:49,904 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
2023-04-10 21:26:49,909 - torch.distributed.distributed_c10d: [    INFO] - Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:26:49,909 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
2023-04-10 21:26:49,909 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 7, total 8.
2023-04-10 21:26:49,910 - torch.distributed.distributed_c10d: [    INFO] - Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:26:49,910 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 2, total 8.
2023-04-10 21:26:49,910 - torch.distributed.distributed_c10d: [    INFO] - Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:26:49,911 - torch.distributed.distributed_c10d: [    INFO] - Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:26:49,911 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 4, total 8.
2023-04-10 21:26:49,911 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 3, total 8.
2023-04-10 21:26:49,912 - torch.distributed.distributed_c10d: [    INFO] - Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:26:49,912 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 1, total 8.
2023-04-10 21:26:49,914 - torch.distributed.distributed_c10d: [    INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:26:49,915 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 0, total 8.
2023-04-10 21:26:49,915 - torch.distributed.distributed_c10d: [    INFO] - Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:26:49,916 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 5, total 8.
2023-04-10 21:26:49,918 - torch.distributed.distributed_c10d: [    INFO] - Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:26:49,919 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 6, total 8.
2023-04-10 21:26:50,325 -                train: [    INFO] - Model vit_snn created, param count: 29705768
2023-04-10 21:26:50,325 -          data.config: [    INFO] - Data processing configuration for current model + dataset:
2023-04-10 21:26:50,325 -          data.config: [    INFO] - 	input_size: (3, 224, 224)
2023-04-10 21:26:50,326 -          data.config: [    INFO] - 	interpolation: bicubic
2023-04-10 21:26:50,326 -          data.config: [    INFO] - 	mean: (0.485, 0.456, 0.406)
2023-04-10 21:26:50,326 -          data.config: [    INFO] - 	std: (0.229, 0.224, 0.225)
2023-04-10 21:26:50,326 -          data.config: [    INFO] - 	crop_pct: 1.0
2023-04-10 21:26:52,812 -                train: [    INFO] - Using native Torch AMP. Training in mixed precision.
2023-04-10 21:26:53,118 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:26:53,176 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-305.pth.tar' (epoch 305)
2023-04-10 21:26:53,531 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:26:53,589 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-290.pth.tar' (epoch 290)
2023-04-10 21:26:53,766 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:26:53,825 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-298.pth.tar' (epoch 298)
2023-04-10 21:26:54,005 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:26:54,062 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-301.pth.tar' (epoch 301)
2023-04-10 21:26:54,242 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:26:54,300 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-308.pth.tar' (epoch 308)
2023-04-10 21:26:54,483 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:26:54,543 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-285.pth.tar' (epoch 285)
2023-04-10 21:26:54,722 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:26:54,780 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-295.pth.tar' (epoch 295)
2023-04-10 21:26:55,024 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:26:55,082 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-300.pth.tar' (epoch 300)
2023-04-10 21:26:55,266 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:26:55,354 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-297.pth.tar' (epoch 297)
2023-04-10 21:26:55,538 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:26:55,594 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-302.pth.tar' (epoch 302)
2023-04-10 21:26:55,614 -                train: [    INFO] - Converted model to use Synchronized BatchNorm. WARNING: You may have issues if using zero initialized BN layers (enabled by default for ResNets) while sync-bn enabled.
2023-04-10 21:26:55,614 -                train: [    INFO] - Using native Torch DistributedDataParallel.
2023-04-10 21:26:57,869 -                train: [    INFO] - Scheduled epochs: 20
2023-04-10 21:27:19,941 -                train: [    INFO] - Test: [   0/97]  Time: 5.663 (5.663)  DataTime: 1.771 (1.771)  Loss:  0.9492 (0.9492)  Acc@1: 82.6172 (82.6172)  Acc@5: 95.8984 (95.8984)
2023-04-10 21:30:37,224 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
2023-04-10 21:30:37,236 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
2023-04-10 21:30:37,257 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
2023-04-10 21:30:37,270 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
2023-04-10 21:30:37,312 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
2023-04-10 21:30:37,313 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
2023-04-10 21:30:37,322 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
2023-04-10 21:30:37,324 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
2023-04-10 21:30:37,324 - torch.distributed.distributed_c10d: [    INFO] - Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:30:37,324 - torch.distributed.distributed_c10d: [    INFO] - Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:30:37,325 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 7, total 8.
2023-04-10 21:30:37,325 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 5, total 8.
2023-04-10 21:30:37,327 - torch.distributed.distributed_c10d: [    INFO] - Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:30:37,328 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 3, total 8.
2023-04-10 21:30:37,328 - torch.distributed.distributed_c10d: [    INFO] - Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:30:37,328 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 1, total 8.
2023-04-10 21:30:37,330 - torch.distributed.distributed_c10d: [    INFO] - Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:30:37,331 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 6, total 8.
2023-04-10 21:30:37,333 - torch.distributed.distributed_c10d: [    INFO] - Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:30:37,333 - torch.distributed.distributed_c10d: [    INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:30:37,333 - torch.distributed.distributed_c10d: [    INFO] - Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:30:37,333 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 4, total 8.
2023-04-10 21:30:37,333 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 0, total 8.
2023-04-10 21:30:37,333 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 2, total 8.
2023-04-10 21:30:37,721 -                train: [    INFO] - Model vit_snn created, param count: 29705768
2023-04-10 21:30:37,721 -          data.config: [    INFO] - Data processing configuration for current model + dataset:
2023-04-10 21:30:37,721 -          data.config: [    INFO] - 	input_size: (3, 224, 224)
2023-04-10 21:30:37,722 -          data.config: [    INFO] - 	interpolation: bicubic
2023-04-10 21:30:37,722 -          data.config: [    INFO] - 	mean: (0.485, 0.456, 0.406)
2023-04-10 21:30:37,722 -          data.config: [    INFO] - 	std: (0.229, 0.224, 0.225)
2023-04-10 21:30:37,722 -          data.config: [    INFO] - 	crop_pct: 1.0
2023-04-10 21:30:40,238 -                train: [    INFO] - Using native Torch AMP. Training in mixed precision.
2023-04-10 21:30:40,513 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:30:40,570 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-305.pth.tar' (epoch 305)
2023-04-10 21:30:40,888 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:30:40,946 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-290.pth.tar' (epoch 290)
2023-04-10 21:30:41,103 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:30:41,166 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-298.pth.tar' (epoch 298)
2023-04-10 21:30:41,324 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:30:41,384 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-301.pth.tar' (epoch 301)
2023-04-10 21:30:41,548 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:30:41,609 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-308.pth.tar' (epoch 308)
2023-04-10 21:30:41,773 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:30:41,834 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-285.pth.tar' (epoch 285)
2023-04-10 21:30:41,997 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:30:42,060 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-295.pth.tar' (epoch 295)
2023-04-10 21:30:42,320 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:30:42,381 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-300.pth.tar' (epoch 300)
2023-04-10 21:30:42,564 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:30:42,780 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-297.pth.tar' (epoch 297)
2023-04-10 21:30:42,988 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:30:43,048 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-302.pth.tar' (epoch 302)
2023-04-10 21:30:43,070 -                train: [    INFO] - Converted model to use Synchronized BatchNorm. WARNING: You may have issues if using zero initialized BN layers (enabled by default for ResNets) while sync-bn enabled.
2023-04-10 21:30:43,071 -                train: [    INFO] - Using native Torch DistributedDataParallel.
2023-04-10 21:30:45,152 -                train: [    INFO] - Scheduled epochs: 20
2023-04-10 21:31:07,748 -                train: [    INFO] - Test: [   0/97]  Time: 5.457 (5.457)  DataTime: 2.408 (2.408)  Loss:  0.9492 (0.9492)  Acc@1: 82.6172 (82.6172)  Acc@5: 95.8984 (95.8984)
2023-04-10 21:32:46,954 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
2023-04-10 21:32:46,963 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
2023-04-10 21:32:46,981 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
2023-04-10 21:32:47,019 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
2023-04-10 21:32:47,019 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
2023-04-10 21:32:47,029 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
2023-04-10 21:32:47,036 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
2023-04-10 21:32:47,038 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
2023-04-10 21:32:47,039 - torch.distributed.distributed_c10d: [    INFO] - Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:32:47,039 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 1, total 8.
2023-04-10 21:32:47,039 - torch.distributed.distributed_c10d: [    INFO] - Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:32:47,040 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 6, total 8.
2023-04-10 21:32:47,040 - torch.distributed.distributed_c10d: [    INFO] - Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:32:47,040 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 2, total 8.
2023-04-10 21:32:47,040 - torch.distributed.distributed_c10d: [    INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:32:47,040 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 0, total 8.
2023-04-10 21:32:47,043 - torch.distributed.distributed_c10d: [    INFO] - Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:32:47,043 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 3, total 8.
2023-04-10 21:32:47,046 - torch.distributed.distributed_c10d: [    INFO] - Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:32:47,046 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 5, total 8.
2023-04-10 21:32:47,047 - torch.distributed.distributed_c10d: [    INFO] - Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:32:47,047 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 7, total 8.
2023-04-10 21:32:47,047 - torch.distributed.distributed_c10d: [    INFO] - Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:32:47,047 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 4, total 8.
2023-04-10 21:32:47,416 -                train: [    INFO] - Model vit_snn created, param count: 29705768
2023-04-10 21:32:47,416 -          data.config: [    INFO] - Data processing configuration for current model + dataset:
2023-04-10 21:32:47,417 -          data.config: [    INFO] - 	input_size: (3, 224, 224)
2023-04-10 21:32:47,417 -          data.config: [    INFO] - 	interpolation: bicubic
2023-04-10 21:32:47,417 -          data.config: [    INFO] - 	mean: (0.485, 0.456, 0.406)
2023-04-10 21:32:47,417 -          data.config: [    INFO] - 	std: (0.229, 0.224, 0.225)
2023-04-10 21:32:47,417 -          data.config: [    INFO] - 	crop_pct: 1.0
2023-04-10 21:32:49,791 -                train: [    INFO] - Using native Torch AMP. Training in mixed precision.
2023-04-10 21:32:50,101 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:32:50,163 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-305.pth.tar' (epoch 305)
2023-04-10 21:32:50,518 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:32:50,576 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-290.pth.tar' (epoch 290)
2023-04-10 21:32:50,755 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:32:50,814 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-298.pth.tar' (epoch 298)
2023-04-10 21:32:50,994 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:32:51,053 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-301.pth.tar' (epoch 301)
2023-04-10 21:32:51,252 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:32:51,313 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-308.pth.tar' (epoch 308)
2023-04-10 21:32:51,512 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:32:51,580 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-285.pth.tar' (epoch 285)
2023-04-10 21:32:51,814 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:32:51,882 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-295.pth.tar' (epoch 295)
2023-04-10 21:32:52,202 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:32:52,270 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-300.pth.tar' (epoch 300)
2023-04-10 21:32:52,499 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:32:52,561 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-297.pth.tar' (epoch 297)
2023-04-10 21:32:52,736 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:32:52,794 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-302.pth.tar' (epoch 302)
2023-04-10 21:32:52,813 -                train: [    INFO] - Converted model to use Synchronized BatchNorm. WARNING: You may have issues if using zero initialized BN layers (enabled by default for ResNets) while sync-bn enabled.
2023-04-10 21:32:52,813 -                train: [    INFO] - Using native Torch DistributedDataParallel.
2023-04-10 21:32:55,047 -                train: [    INFO] - Scheduled epochs: 20
2023-04-10 21:33:16,823 -                train: [    INFO] - Test: [   0/195]  Time: 4.192 (4.192)  DataTime: 1.097 (1.097)  Loss:  0.9805 (0.9805)  Acc@1: 82.0312 (82.0312)  Acc@5: 96.0938 (96.0938)
2023-04-10 21:38:56,807 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
2023-04-10 21:38:56,823 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
2023-04-10 21:38:56,838 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
2023-04-10 21:38:56,854 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
2023-04-10 21:38:56,863 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
2023-04-10 21:38:56,876 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
2023-04-10 21:38:56,896 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
2023-04-10 21:38:56,896 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
2023-04-10 21:38:56,896 - torch.distributed.distributed_c10d: [    INFO] - Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:38:56,896 - torch.distributed.distributed_c10d: [    INFO] - Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:38:56,896 - torch.distributed.distributed_c10d: [    INFO] - Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:38:56,896 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 4, total 8.
2023-04-10 21:38:56,896 - torch.distributed.distributed_c10d: [    INFO] - Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:38:56,896 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 2, total 8.
2023-04-10 21:38:56,896 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 3, total 8.
2023-04-10 21:38:56,897 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 7, total 8.
2023-04-10 21:38:56,897 - torch.distributed.distributed_c10d: [    INFO] - Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:38:56,897 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 6, total 8.
2023-04-10 21:38:56,900 - torch.distributed.distributed_c10d: [    INFO] - Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:38:56,901 - torch.distributed.distributed_c10d: [    INFO] - Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:38:56,901 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 1, total 8.
2023-04-10 21:38:56,901 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 5, total 8.
2023-04-10 21:38:56,905 - torch.distributed.distributed_c10d: [    INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:38:56,905 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 0, total 8.
2023-04-10 21:38:57,309 -                train: [    INFO] - Model vit_snn created, param count: 29705768
2023-04-10 21:38:57,309 -          data.config: [    INFO] - Data processing configuration for current model + dataset:
2023-04-10 21:38:57,309 -          data.config: [    INFO] - 	input_size: (3, 224, 224)
2023-04-10 21:38:57,310 -          data.config: [    INFO] - 	interpolation: bicubic
2023-04-10 21:38:57,310 -          data.config: [    INFO] - 	mean: (0.485, 0.456, 0.406)
2023-04-10 21:38:57,310 -          data.config: [    INFO] - 	std: (0.229, 0.224, 0.225)
2023-04-10 21:38:57,310 -          data.config: [    INFO] - 	crop_pct: 1.0
2023-04-10 21:38:59,729 -                train: [    INFO] - Using native Torch AMP. Training in mixed precision.
2023-04-10 21:39:00,003 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:39:00,056 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-305.pth.tar' (epoch 305)
2023-04-10 21:39:00,377 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:39:00,431 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-290.pth.tar' (epoch 290)
2023-04-10 21:39:00,588 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:39:00,643 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-298.pth.tar' (epoch 298)
2023-04-10 21:39:00,800 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:39:00,860 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-301.pth.tar' (epoch 301)
2023-04-10 21:39:01,024 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:39:01,079 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-308.pth.tar' (epoch 308)
2023-04-10 21:39:01,240 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:39:01,295 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-285.pth.tar' (epoch 285)
2023-04-10 21:39:01,458 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:39:01,511 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-295.pth.tar' (epoch 295)
2023-04-10 21:39:01,791 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:39:01,852 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-300.pth.tar' (epoch 300)
2023-04-10 21:39:02,023 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:39:02,118 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-297.pth.tar' (epoch 297)
2023-04-10 21:39:02,286 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:39:02,339 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-302.pth.tar' (epoch 302)
2023-04-10 21:39:02,358 -                train: [    INFO] - Converted model to use Synchronized BatchNorm. WARNING: You may have issues if using zero initialized BN layers (enabled by default for ResNets) while sync-bn enabled.
2023-04-10 21:39:02,359 -                train: [    INFO] - Using native Torch DistributedDataParallel.
2023-04-10 21:39:03,565 -                train: [    INFO] - Scheduled epochs: 20
2023-04-10 21:39:24,619 -                train: [    INFO] - Test: [   0/195]  Time: 4.415 (4.415)  DataTime: 1.277 (1.277)  Loss:  0.9805 (0.9805)  Acc@1: 82.0312 (82.0312)  Acc@5: 96.0938 (96.0938)
2023-04-10 21:39:38,254 -                train: [    INFO] - Test: [  50/195]  Time: 0.254 (0.354)  DataTime: 0.001 (0.026)  Loss:  6.4648 (6.5061)  Acc@1:  0.7812 ( 4.1360)  Acc@5:  3.1250 ( 9.7963)
2023-04-10 21:39:51,923 -                train: [    INFO] - Test: [ 100/195]  Time: 0.281 (0.314)  DataTime: 0.001 (0.013)  Loss:  7.4688 (6.6856)  Acc@1:  0.7812 ( 2.9780)  Acc@5:  2.3438 ( 7.5727)
2023-04-10 21:40:05,258 -                train: [    INFO] - Test: [ 150/195]  Time: 0.264 (0.298)  DataTime: 0.001 (0.009)  Loss:  6.8633 (6.8442)  Acc@1:  1.1719 ( 2.4602)  Acc@5:  2.7344 ( 6.3923)
2023-04-10 21:45:37,263 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
2023-04-10 21:45:37,277 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
2023-04-10 21:45:37,297 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
2023-04-10 21:45:37,317 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
2023-04-10 21:45:37,336 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
2023-04-10 21:45:37,349 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
2023-04-10 21:45:37,354 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
2023-04-10 21:45:37,356 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
2023-04-10 21:45:37,356 - torch.distributed.distributed_c10d: [    INFO] - Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:45:37,356 - torch.distributed.distributed_c10d: [    INFO] - Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:45:37,356 - torch.distributed.distributed_c10d: [    INFO] - Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:45:37,357 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 7, total 8.
2023-04-10 21:45:37,357 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 3, total 8.
2023-04-10 21:45:37,357 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 2, total 8.
2023-04-10 21:45:37,359 - torch.distributed.distributed_c10d: [    INFO] - Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:45:37,360 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 1, total 8.
2023-04-10 21:45:37,360 - torch.distributed.distributed_c10d: [    INFO] - Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:45:37,360 - torch.distributed.distributed_c10d: [    INFO] - Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:45:37,360 - torch.distributed.distributed_c10d: [    INFO] - Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:45:37,360 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 6, total 8.
2023-04-10 21:45:37,360 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 4, total 8.
2023-04-10 21:45:37,360 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 5, total 8.
2023-04-10 21:45:37,364 - torch.distributed.distributed_c10d: [    INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:45:37,365 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 0, total 8.
2023-04-10 21:45:37,751 -                train: [    INFO] - Model vit_snn created, param count: 29705768
2023-04-10 21:45:37,752 -          data.config: [    INFO] - Data processing configuration for current model + dataset:
2023-04-10 21:45:37,752 -          data.config: [    INFO] - 	input_size: (3, 224, 224)
2023-04-10 21:45:37,752 -          data.config: [    INFO] - 	interpolation: bicubic
2023-04-10 21:45:37,752 -          data.config: [    INFO] - 	mean: (0.485, 0.456, 0.406)
2023-04-10 21:45:37,752 -          data.config: [    INFO] - 	std: (0.229, 0.224, 0.225)
2023-04-10 21:45:37,752 -          data.config: [    INFO] - 	crop_pct: 1.0
2023-04-10 21:45:40,298 -                train: [    INFO] - Using native Torch AMP. Training in mixed precision.
2023-04-10 21:45:40,616 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:45:40,675 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-305.pth.tar' (epoch 305)
2023-04-10 21:45:41,018 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:45:41,095 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-290.pth.tar' (epoch 290)
2023-04-10 21:45:41,263 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:45:41,341 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-298.pth.tar' (epoch 298)
2023-04-10 21:45:41,523 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:45:41,600 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-301.pth.tar' (epoch 301)
2023-04-10 21:45:41,819 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:45:41,885 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-308.pth.tar' (epoch 308)
2023-04-10 21:45:42,086 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:45:42,154 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-285.pth.tar' (epoch 285)
2023-04-10 21:45:42,357 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:45:42,412 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-295.pth.tar' (epoch 295)
2023-04-10 21:45:42,670 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:45:42,752 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-300.pth.tar' (epoch 300)
2023-04-10 21:45:42,946 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:45:42,999 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-297.pth.tar' (epoch 297)
2023-04-10 21:45:43,181 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:45:43,231 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-302.pth.tar' (epoch 302)
2023-04-10 21:45:43,247 -                train: [    INFO] - Converted model to use Synchronized BatchNorm. WARNING: You may have issues if using zero initialized BN layers (enabled by default for ResNets) while sync-bn enabled.
2023-04-10 21:45:43,248 -                train: [    INFO] - Using native Torch DistributedDataParallel.
2023-04-10 21:45:44,282 -                train: [    INFO] - Scheduled epochs: 20
2023-04-10 21:47:24,154 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
2023-04-10 21:47:24,171 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
2023-04-10 21:47:24,187 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
2023-04-10 21:47:24,215 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
2023-04-10 21:47:24,220 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
2023-04-10 21:47:24,238 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
2023-04-10 21:47:24,247 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
2023-04-10 21:47:24,249 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
2023-04-10 21:47:24,250 - torch.distributed.distributed_c10d: [    INFO] - Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:47:24,250 - torch.distributed.distributed_c10d: [    INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:47:24,250 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 0, total 8.
2023-04-10 21:47:24,250 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 7, total 8.
2023-04-10 21:47:24,251 - torch.distributed.distributed_c10d: [    INFO] - Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:47:24,251 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 4, total 8.
2023-04-10 21:47:24,253 - torch.distributed.distributed_c10d: [    INFO] - Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:47:24,254 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 1, total 8.
2023-04-10 21:47:24,256 - torch.distributed.distributed_c10d: [    INFO] - Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:47:24,257 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 5, total 8.
2023-04-10 21:47:24,258 - torch.distributed.distributed_c10d: [    INFO] - Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:47:24,258 - torch.distributed.distributed_c10d: [    INFO] - Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:47:24,258 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 3, total 8.
2023-04-10 21:47:24,258 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 2, total 8.
2023-04-10 21:47:24,259 - torch.distributed.distributed_c10d: [    INFO] - Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:47:24,260 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 6, total 8.
2023-04-10 21:47:24,597 -                train: [    INFO] - Model vit_snn created, param count: 29705768
2023-04-10 21:47:24,598 -          data.config: [    INFO] - Data processing configuration for current model + dataset:
2023-04-10 21:47:24,598 -          data.config: [    INFO] - 	input_size: (3, 224, 224)
2023-04-10 21:47:24,598 -          data.config: [    INFO] - 	interpolation: bicubic
2023-04-10 21:47:24,598 -          data.config: [    INFO] - 	mean: (0.485, 0.456, 0.406)
2023-04-10 21:47:24,598 -          data.config: [    INFO] - 	std: (0.229, 0.224, 0.225)
2023-04-10 21:47:24,598 -          data.config: [    INFO] - 	crop_pct: 1.0
2023-04-10 21:47:26,937 -                train: [    INFO] - Using native Torch AMP. Training in mixed precision.
2023-04-10 21:47:27,204 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:47:27,258 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-305.pth.tar' (epoch 305)
2023-04-10 21:47:27,572 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:47:27,626 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-290.pth.tar' (epoch 290)
2023-04-10 21:47:27,783 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:47:27,838 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-298.pth.tar' (epoch 298)
2023-04-10 21:47:27,996 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:47:28,052 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-301.pth.tar' (epoch 301)
2023-04-10 21:47:28,210 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:47:28,265 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-308.pth.tar' (epoch 308)
2023-04-10 21:47:28,429 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:47:28,484 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-285.pth.tar' (epoch 285)
2023-04-10 21:47:28,646 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:47:28,700 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-295.pth.tar' (epoch 295)
2023-04-10 21:47:28,957 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:47:29,012 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-300.pth.tar' (epoch 300)
2023-04-10 21:47:29,173 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:47:29,228 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-297.pth.tar' (epoch 297)
2023-04-10 21:47:29,392 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:47:29,470 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-302.pth.tar' (epoch 302)
2023-04-10 21:47:29,490 -                train: [    INFO] - Converted model to use Synchronized BatchNorm. WARNING: You may have issues if using zero initialized BN layers (enabled by default for ResNets) while sync-bn enabled.
2023-04-10 21:47:29,491 -                train: [    INFO] - Using native Torch DistributedDataParallel.
2023-04-10 21:47:30,912 -                train: [    INFO] - Scheduled epochs: 20
2023-04-10 21:47:51,509 -                train: [    INFO] - Test: [   0/195]  Time: 3.631 (3.631)  DataTime: 1.352 (1.352)  Loss:  0.9805 (0.9805)  Acc@1: 82.0312 (82.0312)  Acc@5: 96.0938 (96.0938)
2023-04-10 21:48:05,376 -                train: [    INFO] - Test: [  50/195]  Time: 0.339 (0.343)  DataTime: 0.001 (0.027)  Loss:  0.7754 (1.0982)  Acc@1: 87.1094 (77.9182)  Acc@5: 96.4844 (94.2479)
2023-04-10 21:48:19,311 -                train: [    INFO] - Test: [ 100/195]  Time: 0.362 (0.311)  DataTime: 0.001 (0.014)  Loss:  1.7129 (1.1993)  Acc@1: 63.6719 (75.7890)  Acc@5: 88.6719 (93.0654)
2023-04-10 21:48:33,123 -                train: [    INFO] - Test: [ 150/195]  Time: 0.277 (0.300)  DataTime: 0.001 (0.010)  Loss:  1.2969 (1.2972)  Acc@1: 78.9062 (73.9523)  Acc@5: 89.0625 (91.5796)
2023-04-10 21:48:44,298 -                train: [    INFO] - Test: [ 195/195]  Time: 0.099 (0.288)  DataTime: 0.000 (0.008)  Loss:  2.1406 (1.3271)  Acc@1: 41.2500 (73.2220)  Acc@5: 82.5000 (91.2220)
2023-04-10 21:48:45,938 -                train: [    INFO] - Test: [   0/195]  Time: 1.604 (1.604)  DataTime: 1.016 (1.016)  Loss:  1.0205 (1.0205)  Acc@1: 81.6406 (81.6406)  Acc@5: 95.7031 (95.7031)
2023-04-10 21:48:59,617 -                train: [    INFO] - Test: [  50/195]  Time: 0.264 (0.300)  DataTime: 0.001 (0.021)  Loss:  0.8203 (1.1143)  Acc@1: 87.5000 (77.9795)  Acc@5: 96.4844 (94.1406)
2023-04-10 21:49:13,328 -                train: [    INFO] - Test: [ 100/195]  Time: 0.253 (0.287)  DataTime: 0.001 (0.011)  Loss:  1.7012 (1.2098)  Acc@1: 65.6250 (75.8277)  Acc@5: 87.5000 (93.0036)
2023-04-10 21:49:27,139 -                train: [    INFO] - Test: [ 150/195]  Time: 0.250 (0.283)  DataTime: 0.001 (0.008)  Loss:  1.3682 (1.3106)  Acc@1: 77.3438 (73.9497)  Acc@5: 89.0625 (91.4890)
2023-04-10 21:49:38,695 -                train: [    INFO] - Test: [ 195/195]  Time: 0.162 (0.277)  DataTime: 0.000 (0.006)  Loss:  2.2188 (1.3439)  Acc@1: 46.2500 (73.0940)  Acc@5: 82.5000 (91.1360)
2023-04-10 21:49:40,457 -                train: [    INFO] - Test: [   0/195]  Time: 1.726 (1.726)  DataTime: 0.696 (0.696)  Loss:  0.9912 (0.9912)  Acc@1: 80.8594 (80.8594)  Acc@5: 96.0938 (96.0938)
2023-04-10 21:49:53,903 -                train: [    INFO] - Test: [  50/195]  Time: 0.293 (0.297)  DataTime: 0.001 (0.014)  Loss:  0.8330 (1.0750)  Acc@1: 83.2031 (77.9795)  Acc@5: 96.4844 (94.2249)
2023-04-10 21:50:07,461 -                train: [    INFO] - Test: [ 100/195]  Time: 0.293 (0.284)  DataTime: 0.001 (0.008)  Loss:  1.5957 (1.1824)  Acc@1: 66.4062 (75.8006)  Acc@5: 88.6719 (93.0770)
2023-04-10 21:50:20,798 -                train: [    INFO] - Test: [ 150/195]  Time: 0.279 (0.279)  DataTime: 0.001 (0.005)  Loss:  1.2861 (1.2843)  Acc@1: 77.7344 (73.9445)  Acc@5: 89.4531 (91.5356)
2023-04-10 21:50:32,101 -                train: [    INFO] - Test: [ 195/195]  Time: 0.107 (0.272)  DataTime: 0.000 (0.004)  Loss:  2.1758 (1.3181)  Acc@1: 48.7500 (73.2120)  Acc@5: 83.7500 (91.2100)
2023-04-10 21:50:33,810 -                train: [    INFO] - Test: [   0/195]  Time: 1.673 (1.673)  DataTime: 0.761 (0.761)  Loss:  1.0088 (1.0088)  Acc@1: 83.2031 (83.2031)  Acc@5: 94.1406 (94.1406)
2023-04-10 21:50:47,261 -                train: [    INFO] - Test: [  50/195]  Time: 0.261 (0.297)  DataTime: 0.001 (0.016)  Loss:  0.8540 (1.0872)  Acc@1: 84.7656 (77.7803)  Acc@5: 95.3125 (94.1789)
2023-04-10 21:51:00,721 -                train: [    INFO] - Test: [ 100/195]  Time: 0.261 (0.283)  DataTime: 0.001 (0.008)  Loss:  1.5586 (1.1787)  Acc@1: 65.6250 (75.9476)  Acc@5: 90.2344 (93.0925)
2023-04-10 21:51:14,248 -                train: [    INFO] - Test: [ 150/195]  Time: 0.301 (0.279)  DataTime: 0.001 (0.006)  Loss:  1.3789 (1.2775)  Acc@1: 75.7812 (74.0842)  Acc@5: 89.4531 (91.6184)
2023-04-10 21:51:26,126 -                train: [    INFO] - Test: [ 195/195]  Time: 0.143 (0.275)  DataTime: 0.000 (0.005)  Loss:  2.0508 (1.3065)  Acc@1: 50.0000 (73.3340)  Acc@5: 85.0000 (91.2660)
2023-04-10 21:51:27,932 -                train: [    INFO] - Test: [   0/195]  Time: 1.756 (1.756)  DataTime: 0.916 (0.916)  Loss:  1.0547 (1.0547)  Acc@1: 81.2500 (81.2500)  Acc@5: 95.7031 (95.7031)
2023-04-10 21:51:41,518 -                train: [    INFO] - Test: [  50/195]  Time: 0.259 (0.301)  DataTime: 0.001 (0.019)  Loss:  0.8359 (1.1034)  Acc@1: 85.1562 (77.7191)  Acc@5: 95.7031 (94.4164)
2023-04-10 21:51:55,280 -                train: [    INFO] - Test: [ 100/195]  Time: 0.307 (0.288)  DataTime: 0.001 (0.010)  Loss:  1.6289 (1.2032)  Acc@1: 64.8438 (75.7580)  Acc@5: 89.4531 (93.1157)
2023-04-10 21:52:08,316 -                train: [    INFO] - Test: [ 150/195]  Time: 0.264 (0.279)  DataTime: 0.001 (0.007)  Loss:  1.4102 (1.3046)  Acc@1: 75.7812 (73.9911)  Acc@5: 88.2812 (91.5718)
2023-04-10 21:52:20,215 -                train: [    INFO] - Test: [ 195/195]  Time: 0.122 (0.276)  DataTime: 0.000 (0.005)  Loss:  2.2402 (1.3394)  Acc@1: 46.2500 (73.1160)  Acc@5: 78.7500 (91.2300)
2023-04-10 21:52:21,899 -                train: [    INFO] - Test: [   0/195]  Time: 1.648 (1.648)  DataTime: 1.040 (1.040)  Loss:  1.0410 (1.0410)  Acc@1: 78.5156 (78.5156)  Acc@5: 96.4844 (96.4844)
2023-04-10 21:52:35,292 -                train: [    INFO] - Test: [  50/195]  Time: 0.264 (0.295)  DataTime: 0.001 (0.021)  Loss:  0.8853 (1.1069)  Acc@1: 84.7656 (77.8339)  Acc@5: 96.8750 (94.1713)
2023-04-10 21:52:48,679 -                train: [    INFO] - Test: [ 100/195]  Time: 0.287 (0.281)  DataTime: 0.001 (0.011)  Loss:  1.6396 (1.2006)  Acc@1: 65.6250 (75.8625)  Acc@5: 88.2812 (92.9726)
2023-04-10 21:53:50,339 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
2023-04-10 21:53:50,353 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
2023-04-10 21:53:50,382 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
2023-04-10 21:53:50,388 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
2023-04-10 21:53:50,409 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
2023-04-10 21:53:50,424 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
2023-04-10 21:53:50,427 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
2023-04-10 21:53:50,427 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
2023-04-10 21:53:50,427 - torch.distributed.distributed_c10d: [    INFO] - Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:53:50,427 - torch.distributed.distributed_c10d: [    INFO] - Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:53:50,427 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 1, total 8.
2023-04-10 21:53:50,427 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 4, total 8.
2023-04-10 21:53:50,430 - torch.distributed.distributed_c10d: [    INFO] - Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:53:50,430 - torch.distributed.distributed_c10d: [    INFO] - Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:53:50,430 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 3, total 8.
2023-04-10 21:53:50,430 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 7, total 8.
2023-04-10 21:53:50,432 - torch.distributed.distributed_c10d: [    INFO] - Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:53:50,432 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 6, total 8.
2023-04-10 21:53:50,434 - torch.distributed.distributed_c10d: [    INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:53:50,434 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 0, total 8.
2023-04-10 21:53:50,435 - torch.distributed.distributed_c10d: [    INFO] - Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:53:50,435 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 5, total 8.
2023-04-10 21:53:50,436 - torch.distributed.distributed_c10d: [    INFO] - Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:53:50,436 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 2, total 8.
2023-04-10 21:53:50,835 -                train: [    INFO] - Model vit_snn created, param count: 29705768
2023-04-10 21:53:50,835 -          data.config: [    INFO] - Data processing configuration for current model + dataset:
2023-04-10 21:53:50,835 -          data.config: [    INFO] - 	input_size: (3, 224, 224)
2023-04-10 21:53:50,835 -          data.config: [    INFO] - 	interpolation: bicubic
2023-04-10 21:53:50,835 -          data.config: [    INFO] - 	mean: (0.485, 0.456, 0.406)
2023-04-10 21:53:50,836 -          data.config: [    INFO] - 	std: (0.229, 0.224, 0.225)
2023-04-10 21:53:50,836 -          data.config: [    INFO] - 	crop_pct: 1.0
2023-04-10 21:53:53,162 -                train: [    INFO] - Using native Torch AMP. Training in mixed precision.
2023-04-10 21:53:53,429 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:53:53,525 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-305.pth.tar' (epoch 305)
2023-04-10 21:53:53,873 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:53:53,931 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-290.pth.tar' (epoch 290)
2023-04-10 21:53:54,096 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:53:54,155 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-298.pth.tar' (epoch 298)
2023-04-10 21:53:54,316 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:53:54,371 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-301.pth.tar' (epoch 301)
2023-04-10 21:53:54,528 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:53:54,584 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-308.pth.tar' (epoch 308)
2023-04-10 21:53:54,748 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:53:54,802 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-285.pth.tar' (epoch 285)
2023-04-10 21:53:54,963 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:53:55,015 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-295.pth.tar' (epoch 295)
2023-04-10 21:53:55,272 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:53:55,326 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-300.pth.tar' (epoch 300)
2023-04-10 21:53:55,487 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:53:55,542 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-297.pth.tar' (epoch 297)
2023-04-10 21:53:55,702 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:53:55,770 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-302.pth.tar' (epoch 302)
2023-04-10 21:53:55,788 -                train: [    INFO] - Converted model to use Synchronized BatchNorm. WARNING: You may have issues if using zero initialized BN layers (enabled by default for ResNets) while sync-bn enabled.
2023-04-10 21:53:55,789 -                train: [    INFO] - Using native Torch DistributedDataParallel.
2023-04-10 21:53:58,224 -                train: [    INFO] - Scheduled epochs: 20
2023-04-10 21:54:21,488 -                train: [    INFO] - Test: [   0/78]  Time: 6.238 (6.238)  DataTime: 2.714 (2.714)  Loss:  0.8672 (0.8672)  Acc@1: 84.5312 (84.5312)  Acc@5: 96.4062 (96.4062)
2023-04-10 21:54:50,409 -                train: [    INFO] - Test: [  50/78]  Time: 0.557 (0.689)  DataTime: 0.001 (0.054)  Loss:  1.5312 (1.2511)  Acc@1: 71.7188 (74.9142)  Acc@5: 85.7812 (92.2059)
2023-04-10 21:55:04,967 -                train: [    INFO] - Test: [  78/78]  Time: 0.102 (0.629)  DataTime: 0.000 (0.035)  Loss:  2.1406 (1.3271)  Acc@1: 41.2500 (73.2220)  Acc@5: 82.5000 (91.2220)
2023-04-10 21:55:08,454 -                train: [    INFO] - Test: [   0/78]  Time: 3.450 (3.450)  DataTime: 2.350 (2.350)  Loss:  0.9546 (0.9546)  Acc@1: 84.0625 (84.0625)  Acc@5: 95.3125 (95.3125)
2023-04-10 21:55:38,874 -                train: [    INFO] - Test: [  50/78]  Time: 0.624 (0.664)  DataTime: 0.001 (0.047)  Loss:  1.5234 (1.2623)  Acc@1: 73.1250 (74.8775)  Acc@5: 85.9375 (92.1324)
2023-04-10 21:55:53,544 -                train: [    INFO] - Test: [  78/78]  Time: 0.101 (0.614)  DataTime: 0.000 (0.031)  Loss:  2.2188 (1.3441)  Acc@1: 46.2500 (73.0940)  Acc@5: 82.5000 (91.1360)
2023-04-10 21:55:57,116 -                train: [    INFO] - Test: [   0/78]  Time: 3.536 (3.536)  DataTime: 1.776 (1.776)  Loss:  0.9092 (0.9092)  Acc@1: 83.2812 (83.2812)  Acc@5: 96.2500 (96.2500)
2023-04-10 21:56:40,708 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
2023-04-10 21:56:40,727 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
2023-04-10 21:56:40,738 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
2023-04-10 21:56:40,764 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
2023-04-10 21:56:40,780 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
2023-04-10 21:56:40,795 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
2023-04-10 21:56:40,805 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
2023-04-10 21:56:40,810 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
2023-04-10 21:56:40,810 - torch.distributed.distributed_c10d: [    INFO] - Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:56:40,811 - torch.distributed.distributed_c10d: [    INFO] - Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:56:40,811 - torch.distributed.distributed_c10d: [    INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:56:40,811 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 3, total 8.
2023-04-10 21:56:40,811 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 7, total 8.
2023-04-10 21:56:40,811 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 0, total 8.
2023-04-10 21:56:40,813 - torch.distributed.distributed_c10d: [    INFO] - Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:56:40,813 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 5, total 8.
2023-04-10 21:56:40,816 - torch.distributed.distributed_c10d: [    INFO] - Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:56:40,816 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 2, total 8.
2023-04-10 21:56:40,816 - torch.distributed.distributed_c10d: [    INFO] - Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:56:40,816 - torch.distributed.distributed_c10d: [    INFO] - Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:56:40,817 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 1, total 8.
2023-04-10 21:56:40,817 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 6, total 8.
2023-04-10 21:56:40,820 - torch.distributed.distributed_c10d: [    INFO] - Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 21:56:40,821 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 4, total 8.
2023-04-10 21:56:41,140 -                train: [    INFO] - Model vit_snn created, param count: 29705768
2023-04-10 21:56:41,140 -          data.config: [    INFO] - Data processing configuration for current model + dataset:
2023-04-10 21:56:41,140 -          data.config: [    INFO] - 	input_size: (3, 224, 224)
2023-04-10 21:56:41,140 -          data.config: [    INFO] - 	interpolation: bicubic
2023-04-10 21:56:41,140 -          data.config: [    INFO] - 	mean: (0.485, 0.456, 0.406)
2023-04-10 21:56:41,140 -          data.config: [    INFO] - 	std: (0.229, 0.224, 0.225)
2023-04-10 21:56:41,141 -          data.config: [    INFO] - 	crop_pct: 1.0
2023-04-10 21:56:43,508 -                train: [    INFO] - Using native Torch AMP. Training in mixed precision.
2023-04-10 21:56:43,764 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:56:43,817 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-305.pth.tar' (epoch 305)
2023-04-10 21:56:44,121 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:56:44,174 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-290.pth.tar' (epoch 290)
2023-04-10 21:56:44,491 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:56:44,546 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-298.pth.tar' (epoch 298)
2023-04-10 21:56:44,878 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:56:44,934 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-301.pth.tar' (epoch 301)
2023-04-10 21:56:45,263 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:56:45,318 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-308.pth.tar' (epoch 308)
2023-04-10 21:56:45,643 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:56:45,725 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-285.pth.tar' (epoch 285)
2023-04-10 21:56:46,056 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:56:46,111 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-295.pth.tar' (epoch 295)
2023-04-10 21:56:46,479 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:56:46,528 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-300.pth.tar' (epoch 300)
2023-04-10 21:56:46,817 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:56:46,866 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-297.pth.tar' (epoch 297)
2023-04-10 21:56:47,151 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 21:56:47,199 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-302.pth.tar' (epoch 302)
2023-04-10 21:56:47,248 -                train: [    INFO] - Converted model to use Synchronized BatchNorm. WARNING: You may have issues if using zero initialized BN layers (enabled by default for ResNets) while sync-bn enabled.
2023-04-10 21:56:47,248 -                train: [    INFO] - Using native Torch DistributedDataParallel.
2023-04-10 21:56:48,971 -                train: [    INFO] - Scheduled epochs: 20
2023-04-10 21:57:13,021 -                train: [    INFO] - Test: [   0/62]  Time: 7.390 (7.390)  DataTime: 2.993 (2.993)  Loss:  0.7939 (0.7939)  Acc@1: 86.0000 (86.0000)  Acc@5: 97.0000 (97.0000)
2023-04-10 21:57:49,962 -                train: [    INFO] - Test: [  50/62]  Time: 0.766 (0.869)  DataTime: 0.000 (0.059)  Loss:  1.3848 (1.3075)  Acc@1: 72.0000 (73.7525)  Acc@5: 90.1250 (91.4191)
2023-04-10 21:57:58,719 -                train: [    INFO] - Test: [  62/62]  Time: 0.396 (0.843)  DataTime: 0.000 (0.048)  Loss:  1.0898 (1.3271)  Acc@1: 76.5000 (73.2200)  Acc@5: 94.2500 (91.2220)
2023-04-10 21:58:03,801 -                train: [    INFO] - Test: [   0/62]  Time: 5.045 (5.045)  DataTime: 2.593 (2.593)  Loss:  0.8784 (0.8784)  Acc@1: 85.5000 (85.5000)  Acc@5: 95.8750 (95.8750)
2023-04-10 21:58:39,415 -                train: [    INFO] - Test: [  50/62]  Time: 0.635 (0.797)  DataTime: 0.000 (0.052)  Loss:  1.4199 (1.3210)  Acc@1: 72.7500 (73.7549)  Acc@5: 89.7500 (91.3113)
2023-04-10 21:58:46,817 -                train: [    INFO] - Test: [  62/62]  Time: 0.334 (0.763)  DataTime: 0.000 (0.042)  Loss:  1.1436 (1.3439)  Acc@1: 76.5000 (73.0920)  Acc@5: 93.5000 (91.1380)
2023-04-10 21:58:51,338 -                train: [    INFO] - Test: [   0/62]  Time: 4.485 (4.485)  DataTime: 2.739 (2.739)  Loss:  0.8408 (0.8408)  Acc@1: 84.8750 (84.8750)  Acc@5: 96.7500 (96.7500)
2023-04-10 21:59:27,997 -                train: [    INFO] - Test: [  50/62]  Time: 0.632 (0.807)  DataTime: 0.000 (0.055)  Loss:  1.4473 (1.2977)  Acc@1: 71.7500 (73.7353)  Acc@5: 90.0000 (91.3652)
2023-04-10 21:59:35,350 -                train: [    INFO] - Test: [  62/62]  Time: 0.330 (0.770)  DataTime: 0.000 (0.045)  Loss:  1.0605 (1.3181)  Acc@1: 80.0000 (73.2160)  Acc@5: 93.7500 (91.2100)
2023-04-10 21:59:40,139 -                train: [    INFO] - Test: [   0/62]  Time: 4.753 (4.753)  DataTime: 2.509 (2.509)  Loss:  0.8247 (0.8247)  Acc@1: 85.8750 (85.8750)  Acc@5: 95.8750 (95.8750)
2023-04-10 22:00:18,043 -                train: [    INFO] - Test: [  50/62]  Time: 0.757 (0.836)  DataTime: 0.000 (0.050)  Loss:  1.3711 (1.2879)  Acc@1: 72.7500 (73.8922)  Acc@5: 89.5000 (91.4436)
2023-04-10 22:00:26,715 -                train: [    INFO] - Test: [  62/62]  Time: 0.353 (0.815)  DataTime: 0.000 (0.041)  Loss:  1.0811 (1.3065)  Acc@1: 78.2500 (73.3360)  Acc@5: 93.7500 (91.2680)
2023-04-10 22:00:31,230 -                train: [    INFO] - Test: [   0/62]  Time: 4.478 (4.478)  DataTime: 3.362 (3.362)  Loss:  0.8169 (0.8169)  Acc@1: 86.1250 (86.1250)  Acc@5: 97.1250 (97.1250)
2023-04-10 22:01:06,704 -                train: [    INFO] - Test: [  50/62]  Time: 0.625 (0.783)  DataTime: 0.000 (0.067)  Loss:  1.4531 (1.3171)  Acc@1: 70.6250 (73.7451)  Acc@5: 90.2500 (91.3799)
2023-04-10 22:01:14,121 -                train: [    INFO] - Test: [  62/62]  Time: 0.331 (0.752)  DataTime: 0.000 (0.054)  Loss:  1.1445 (1.3394)  Acc@1: 77.2500 (73.1180)  Acc@5: 92.0000 (91.2300)
2023-04-10 22:01:18,867 -                train: [    INFO] - Test: [   0/62]  Time: 4.709 (4.709)  DataTime: 3.075 (3.075)  Loss:  0.8350 (0.8350)  Acc@1: 85.3750 (85.3750)  Acc@5: 97.2500 (97.2500)
2023-04-10 22:01:55,322 -                train: [    INFO] - Test: [  50/62]  Time: 0.636 (0.807)  DataTime: 0.000 (0.061)  Loss:  1.3916 (1.3108)  Acc@1: 72.1250 (73.8652)  Acc@5: 90.7500 (91.3627)
2023-04-10 22:02:02,733 -                train: [    INFO] - Test: [  62/62]  Time: 0.333 (0.771)  DataTime: 0.000 (0.050)  Loss:  1.0654 (1.3288)  Acc@1: 79.5000 (73.3300)  Acc@5: 93.5000 (91.2340)
2023-04-10 22:02:07,491 -                train: [    INFO] - Test: [   0/62]  Time: 4.721 (4.721)  DataTime: 3.338 (3.338)  Loss:  0.8403 (0.8403)  Acc@1: 85.8750 (85.8750)  Acc@5: 97.1250 (97.1250)
2023-04-10 22:02:43,401 -                train: [    INFO] - Test: [  50/62]  Time: 0.629 (0.797)  DataTime: 0.000 (0.066)  Loss:  1.4785 (1.3286)  Acc@1: 71.8750 (73.8775)  Acc@5: 89.6250 (91.6078)
2023-04-10 22:02:50,664 -                train: [    INFO] - Test: [  62/62]  Time: 0.329 (0.760)  DataTime: 0.000 (0.054)  Loss:  1.1992 (1.3493)  Acc@1: 75.7500 (73.3020)  Acc@5: 92.0000 (91.4000)
2023-04-10 22:02:55,276 -                train: [    INFO] - Test: [   0/62]  Time: 4.575 (4.575)  DataTime: 3.236 (3.236)  Loss:  0.8071 (0.8071)  Acc@1: 85.3750 (85.3750)  Acc@5: 96.5000 (96.5000)
2023-04-10 22:03:31,891 -                train: [    INFO] - Test: [  50/62]  Time: 0.758 (0.808)  DataTime: 0.000 (0.064)  Loss:  1.3818 (1.2847)  Acc@1: 72.0000 (73.6838)  Acc@5: 90.2500 (91.3505)
2023-04-10 22:03:40,521 -                train: [    INFO] - Test: [  62/62]  Time: 0.390 (0.791)  DataTime: 0.000 (0.052)  Loss:  1.0605 (1.3036)  Acc@1: 79.2500 (73.1600)  Acc@5: 93.0000 (91.2340)
2023-04-10 22:03:45,280 -                train: [    INFO] - Test: [   0/62]  Time: 4.722 (4.722)  DataTime: 3.038 (3.038)  Loss:  0.8433 (0.8433)  Acc@1: 84.8750 (84.8750)  Acc@5: 96.7500 (96.7500)
2023-04-10 22:04:20,188 -                train: [    INFO] - Test: [  50/62]  Time: 0.639 (0.777)  DataTime: 0.000 (0.060)  Loss:  1.4414 (1.3008)  Acc@1: 70.6250 (73.8382)  Acc@5: 89.6250 (91.3382)
2023-04-10 22:04:27,541 -                train: [    INFO] - Test: [  62/62]  Time: 0.332 (0.746)  DataTime: 0.000 (0.049)  Loss:  1.1133 (1.3191)  Acc@1: 79.5000 (73.3180)  Acc@5: 92.7500 (91.2160)
2023-04-10 22:04:32,179 -                train: [    INFO] - Test: [   0/62]  Time: 4.601 (4.601)  DataTime: 2.786 (2.786)  Loss:  0.8389 (0.8389)  Acc@1: 86.6250 (86.6250)  Acc@5: 96.1250 (96.1250)
2023-04-10 22:05:07,675 -                train: [    INFO] - Test: [  50/62]  Time: 0.626 (0.786)  DataTime: 0.000 (0.055)  Loss:  1.4180 (1.3193)  Acc@1: 72.7500 (73.9044)  Acc@5: 90.1250 (91.4044)
2023-04-10 22:05:14,850 -                train: [    INFO] - Test: [  62/62]  Time: 0.326 (0.750)  DataTime: 0.000 (0.045)  Loss:  1.1406 (1.3402)  Acc@1: 78.5000 (73.2800)  Acc@5: 92.5000 (91.2720)
2023-04-10 22:08:40,998 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
2023-04-10 22:08:41,009 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
2023-04-10 22:08:41,034 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
2023-04-10 22:08:41,079 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
2023-04-10 22:08:41,082 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
2023-04-10 22:08:41,094 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
2023-04-10 22:08:41,095 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
2023-04-10 22:08:41,095 - torch.distributed.distributed_c10d: [    INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 22:08:41,095 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
2023-04-10 22:08:41,096 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 0, total 8.
2023-04-10 22:08:41,096 - torch.distributed.distributed_c10d: [    INFO] - Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 22:08:41,096 - torch.distributed.distributed_c10d: [    INFO] - Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 22:08:41,096 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 2, total 8.
2023-04-10 22:08:41,097 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 7, total 8.
2023-04-10 22:08:41,100 - torch.distributed.distributed_c10d: [    INFO] - Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 22:08:41,100 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 5, total 8.
2023-04-10 22:08:41,101 - torch.distributed.distributed_c10d: [    INFO] - Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 22:08:41,101 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 4, total 8.
2023-04-10 22:08:41,102 - torch.distributed.distributed_c10d: [    INFO] - Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 22:08:41,103 - torch.distributed.distributed_c10d: [    INFO] - Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 22:08:41,103 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 3, total 8.
2023-04-10 22:08:41,103 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 1, total 8.
2023-04-10 22:08:41,105 - torch.distributed.distributed_c10d: [    INFO] - Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 22:08:41,105 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 6, total 8.
2023-04-10 22:08:41,498 -                train: [    INFO] - Model vit_snn created, param count: 29705768
2023-04-10 22:08:41,498 -          data.config: [    INFO] - Data processing configuration for current model + dataset:
2023-04-10 22:08:41,498 -          data.config: [    INFO] - 	input_size: (3, 224, 224)
2023-04-10 22:08:41,498 -          data.config: [    INFO] - 	interpolation: bicubic
2023-04-10 22:08:41,498 -          data.config: [    INFO] - 	mean: (0.485, 0.456, 0.406)
2023-04-10 22:08:41,498 -          data.config: [    INFO] - 	std: (0.229, 0.224, 0.225)
2023-04-10 22:08:41,499 -          data.config: [    INFO] - 	crop_pct: 1.0
2023-04-10 22:08:43,807 -                train: [    INFO] - Using native Torch AMP. Training in mixed precision.
2023-04-10 22:08:44,087 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 22:08:44,140 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-305.pth.tar' (epoch 305)
2023-04-10 22:08:44,473 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 22:08:44,527 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-290.pth.tar' (epoch 290)
2023-04-10 22:08:44,682 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 22:08:44,765 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-298.pth.tar' (epoch 298)
2023-04-10 22:08:44,995 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 22:08:45,074 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-301.pth.tar' (epoch 301)
2023-04-10 22:08:45,300 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 22:08:45,383 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-308.pth.tar' (epoch 308)
2023-04-10 22:08:45,607 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 22:08:45,682 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-285.pth.tar' (epoch 285)
2023-04-10 22:08:45,896 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 22:08:45,972 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-295.pth.tar' (epoch 295)
2023-04-10 22:08:46,266 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 22:08:46,472 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-300.pth.tar' (epoch 300)
2023-04-10 22:08:46,695 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 22:08:46,752 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-297.pth.tar' (epoch 297)
2023-04-10 22:08:46,933 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 22:08:46,988 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-302.pth.tar' (epoch 302)
2023-04-10 22:08:47,005 -                train: [    INFO] - Converted model to use Synchronized BatchNorm. WARNING: You may have issues if using zero initialized BN layers (enabled by default for ResNets) while sync-bn enabled.
2023-04-10 22:08:47,005 -                train: [    INFO] - Using native Torch DistributedDataParallel.
2023-04-10 22:08:48,936 -                train: [    INFO] - Scheduled epochs: 20
2023-04-10 22:09:13,468 -                train: [    INFO] - Test: [   0/62]  Time: 6.780 (6.780)  DataTime: 3.680 (3.680)  Loss:  0.7939 (0.7939)  Acc@1: 86.0000 (86.0000)  Acc@5: 97.0000 (97.0000)
2023-04-10 22:09:49,408 -                train: [    INFO] - Test: [  50/62]  Time: 0.649 (0.838)  DataTime: 0.000 (0.073)  Loss:  1.3848 (1.3075)  Acc@1: 72.0000 (73.7525)  Acc@5: 90.1250 (91.4191)
2023-04-10 22:09:57,692 -                train: [    INFO] - Test: [  62/62]  Time: 0.403 (0.810)  DataTime: 0.000 (0.059)  Loss:  1.0898 (1.3271)  Acc@1: 76.5000 (73.2200)  Acc@5: 94.2500 (91.2220)
2023-04-10 22:10:02,356 -                train: [    INFO] - Test: [   0/62]  Time: 4.612 (4.612)  DataTime: 3.332 (3.332)  Loss:  0.8784 (0.8784)  Acc@1: 85.5000 (85.5000)  Acc@5: 95.8750 (95.8750)
2023-04-10 22:10:40,119 -                train: [    INFO] - Test: [  50/62]  Time: 0.759 (0.831)  DataTime: 0.000 (0.067)  Loss:  1.4199 (1.3210)  Acc@1: 72.7500 (73.7549)  Acc@5: 89.7500 (91.3113)
2023-04-10 22:10:47,949 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
2023-04-10 22:10:47,971 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
2023-04-10 22:10:47,992 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
2023-04-10 22:10:48,015 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
2023-04-10 22:10:48,025 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
2023-04-10 22:10:48,038 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
2023-04-10 22:10:48,048 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
2023-04-10 22:10:48,051 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
2023-04-10 22:10:48,052 - torch.distributed.distributed_c10d: [    INFO] - Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 22:10:48,052 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 1, total 8.
2023-04-10 22:10:48,052 - torch.distributed.distributed_c10d: [    INFO] - Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 22:10:48,053 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 2, total 8.
2023-04-10 22:10:48,054 - torch.distributed.distributed_c10d: [    INFO] - Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 22:10:48,054 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 5, total 8.
2023-04-10 22:10:48,054 - torch.distributed.distributed_c10d: [    INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 22:10:48,055 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 0, total 8.
2023-04-10 22:10:48,056 - torch.distributed.distributed_c10d: [    INFO] - Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 22:10:48,057 - torch.distributed.distributed_c10d: [    INFO] - Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 22:10:48,057 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 3, total 8.
2023-04-10 22:10:48,057 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 4, total 8.
2023-04-10 22:10:48,058 - torch.distributed.distributed_c10d: [    INFO] - Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 22:10:48,059 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 6, total 8.
2023-04-10 22:10:48,059 - torch.distributed.distributed_c10d: [    INFO] - Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 22:10:48,060 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 7, total 8.
2023-04-10 22:10:48,462 -                train: [    INFO] - Model vit_snn created, param count: 29705768
2023-04-10 22:10:48,463 -          data.config: [    INFO] - Data processing configuration for current model + dataset:
2023-04-10 22:10:48,463 -          data.config: [    INFO] - 	input_size: (3, 224, 224)
2023-04-10 22:10:48,463 -          data.config: [    INFO] - 	interpolation: bicubic
2023-04-10 22:10:48,463 -          data.config: [    INFO] - 	mean: (0.485, 0.456, 0.406)
2023-04-10 22:10:48,463 -          data.config: [    INFO] - 	std: (0.229, 0.224, 0.225)
2023-04-10 22:10:48,463 -          data.config: [    INFO] - 	crop_pct: 1.0
2023-04-10 22:10:50,697 -                train: [    INFO] - Using native Torch AMP. Training in mixed precision.
2023-04-10 22:10:50,949 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 22:10:51,007 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-305.pth.tar' (epoch 305)
2023-04-10 22:10:51,310 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 22:10:51,367 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-290.pth.tar' (epoch 290)
2023-04-10 22:10:51,522 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 22:10:51,580 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-298.pth.tar' (epoch 298)
2023-04-10 22:10:51,737 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 22:10:51,794 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-301.pth.tar' (epoch 301)
2023-04-10 22:10:51,952 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 22:10:52,010 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-308.pth.tar' (epoch 308)
2023-04-10 22:10:52,171 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 22:10:52,229 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-285.pth.tar' (epoch 285)
2023-04-10 22:10:52,391 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 22:10:52,449 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-295.pth.tar' (epoch 295)
2023-04-10 22:10:52,699 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 22:10:52,758 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-300.pth.tar' (epoch 300)
2023-04-10 22:10:52,920 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 22:10:52,978 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-297.pth.tar' (epoch 297)
2023-04-10 22:10:53,141 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 22:10:53,222 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-302.pth.tar' (epoch 302)
2023-04-10 22:10:53,242 -                train: [    INFO] - Converted model to use Synchronized BatchNorm. WARNING: You may have issues if using zero initialized BN layers (enabled by default for ResNets) while sync-bn enabled.
2023-04-10 22:10:53,242 -                train: [    INFO] - Using native Torch DistributedDataParallel.
2023-04-10 22:10:55,908 -                train: [    INFO] - Scheduled epochs: 20
2023-04-10 22:11:20,364 -                train: [    INFO] - Test: [   0/62]  Time: 8.230 (8.230)  DataTime: 2.354 (2.354)  Loss:  0.7939 (0.7939)  Acc@1: 86.0000 (86.0000)  Acc@5: 97.0000 (97.0000)
2023-04-10 22:11:56,415 -                train: [    INFO] - Test: [  50/62]  Time: 0.745 (0.868)  DataTime: 0.000 (0.047)  Loss:  1.3848 (1.3075)  Acc@1: 72.0000 (73.7525)  Acc@5: 90.1250 (91.4191)
2023-04-10 22:12:04,392 -                train: [    INFO] - Test: [  62/62]  Time: 0.328 (0.829)  DataTime: 0.000 (0.038)  Loss:  1.0898 (1.3271)  Acc@1: 76.5000 (73.2200)  Acc@5: 94.2500 (91.2220)
2023-04-10 22:12:08,976 -                train: [    INFO] - Test: [   0/62]  Time: 4.548 (4.548)  DataTime: 2.395 (2.395)  Loss:  0.8784 (0.8784)  Acc@1: 85.5000 (85.5000)  Acc@5: 95.8750 (95.8750)
2023-04-10 22:12:45,032 -                train: [    INFO] - Test: [  50/62]  Time: 0.633 (0.796)  DataTime: 0.000 (0.048)  Loss:  1.4199 (1.3210)  Acc@1: 72.7500 (73.7549)  Acc@5: 89.7500 (91.3113)
2023-04-10 22:12:52,329 -                train: [    INFO] - Test: [  62/62]  Time: 0.330 (0.760)  DataTime: 0.000 (0.039)  Loss:  1.1436 (1.3439)  Acc@1: 76.5000 (73.0920)  Acc@5: 93.5000 (91.1380)
2023-04-10 22:12:56,801 -                train: [    INFO] - Test: [   0/62]  Time: 4.435 (4.435)  DataTime: 2.955 (2.955)  Loss:  0.8408 (0.8408)  Acc@1: 84.8750 (84.8750)  Acc@5: 96.7500 (96.7500)
2023-04-10 22:13:32,962 -                train: [    INFO] - Test: [  50/62]  Time: 0.656 (0.796)  DataTime: 0.001 (0.059)  Loss:  1.4473 (1.2977)  Acc@1: 71.7500 (73.7353)  Acc@5: 90.0000 (91.3652)
2023-04-10 22:13:40,316 -                train: [    INFO] - Test: [  62/62]  Time: 0.335 (0.761)  DataTime: 0.000 (0.048)  Loss:  1.0605 (1.3181)  Acc@1: 80.0000 (73.2160)  Acc@5: 93.7500 (91.2100)
2023-04-10 22:13:44,702 -                train: [    INFO] - Test: [   0/62]  Time: 4.350 (4.350)  DataTime: 2.527 (2.527)  Loss:  0.8247 (0.8247)  Acc@1: 85.8750 (85.8750)  Acc@5: 95.8750 (95.8750)
2023-04-10 22:14:20,698 -                train: [    INFO] - Test: [  50/62]  Time: 0.750 (0.791)  DataTime: 0.000 (0.051)  Loss:  1.3711 (1.2879)  Acc@1: 72.7500 (73.8922)  Acc@5: 89.5000 (91.4436)
2023-04-10 22:14:28,213 -                train: [    INFO] - Test: [  62/62]  Time: 0.332 (0.760)  DataTime: 0.000 (0.041)  Loss:  1.0811 (1.3065)  Acc@1: 78.2500 (73.3360)  Acc@5: 93.7500 (91.2680)
2023-04-10 22:14:32,824 -                train: [    INFO] - Test: [   0/62]  Time: 4.575 (4.575)  DataTime: 2.392 (2.392)  Loss:  0.8169 (0.8169)  Acc@1: 86.1250 (86.1250)  Acc@5: 97.1250 (97.1250)
2023-04-10 22:15:09,802 -                train: [    INFO] - Test: [  50/62]  Time: 0.637 (0.815)  DataTime: 0.000 (0.048)  Loss:  1.4531 (1.3171)  Acc@1: 70.6250 (73.7451)  Acc@5: 90.2500 (91.3799)
2023-04-10 22:15:17,205 -                train: [    INFO] - Test: [  62/62]  Time: 0.335 (0.777)  DataTime: 0.000 (0.039)  Loss:  1.1445 (1.3394)  Acc@1: 77.2500 (73.1180)  Acc@5: 92.0000 (91.2300)
2023-04-10 22:15:21,625 -                train: [    INFO] - Test: [   0/62]  Time: 4.384 (4.384)  DataTime: 2.439 (2.439)  Loss:  0.8350 (0.8350)  Acc@1: 85.3750 (85.3750)  Acc@5: 97.2500 (97.2500)
2023-04-10 22:15:58,140 -                train: [    INFO] - Test: [  50/62]  Time: 0.636 (0.802)  DataTime: 0.000 (0.049)  Loss:  1.3916 (1.3108)  Acc@1: 72.1250 (73.8652)  Acc@5: 90.7500 (91.3627)
2023-04-10 22:16:06,438 -                train: [    INFO] - Test: [  62/62]  Time: 0.397 (0.781)  DataTime: 0.000 (0.040)  Loss:  1.0654 (1.3288)  Acc@1: 79.5000 (73.3300)  Acc@5: 93.5000 (91.2340)
2023-04-10 22:16:11,041 -                train: [    INFO] - Test: [   0/62]  Time: 4.567 (4.567)  DataTime: 2.975 (2.975)  Loss:  0.8403 (0.8403)  Acc@1: 85.8750 (85.8750)  Acc@5: 97.1250 (97.1250)
2023-04-10 22:16:47,773 -                train: [    INFO] - Test: [  50/62]  Time: 0.630 (0.810)  DataTime: 0.000 (0.060)  Loss:  1.4785 (1.3286)  Acc@1: 71.8750 (73.8775)  Acc@5: 89.6250 (91.6078)
2023-04-10 22:16:55,168 -                train: [    INFO] - Test: [  62/62]  Time: 0.329 (0.773)  DataTime: 0.000 (0.048)  Loss:  1.1992 (1.3493)  Acc@1: 75.7500 (73.3020)  Acc@5: 92.0000 (91.4000)
2023-04-10 22:16:59,966 -                train: [    INFO] - Test: [   0/62]  Time: 4.762 (4.762)  DataTime: 3.473 (3.473)  Loss:  0.8071 (0.8071)  Acc@1: 85.3750 (85.3750)  Acc@5: 96.5000 (96.5000)
2023-04-10 22:17:37,703 -                train: [    INFO] - Test: [  50/62]  Time: 0.750 (0.833)  DataTime: 0.000 (0.069)  Loss:  1.3818 (1.2847)  Acc@1: 72.0000 (73.6838)  Acc@5: 90.2500 (91.3505)
2023-04-10 22:17:46,320 -                train: [    INFO] - Test: [  62/62]  Time: 0.391 (0.811)  DataTime: 0.000 (0.056)  Loss:  1.0605 (1.3036)  Acc@1: 79.2500 (73.1600)  Acc@5: 93.0000 (91.2340)
2023-04-10 22:17:50,826 -                train: [    INFO] - Test: [   0/62]  Time: 4.469 (4.469)  DataTime: 2.982 (2.982)  Loss:  0.8433 (0.8433)  Acc@1: 84.8750 (84.8750)  Acc@5: 96.7500 (96.7500)
2023-04-10 22:18:29,088 -                train: [    INFO] - Test: [  50/62]  Time: 0.745 (0.838)  DataTime: 0.000 (0.059)  Loss:  1.4414 (1.3008)  Acc@1: 70.6250 (73.8382)  Acc@5: 89.6250 (91.3382)
2023-04-10 22:18:37,685 -                train: [    INFO] - Test: [  62/62]  Time: 0.391 (0.815)  DataTime: 0.000 (0.048)  Loss:  1.1133 (1.3191)  Acc@1: 79.5000 (73.3180)  Acc@5: 92.7500 (91.2160)
2023-04-10 22:18:42,490 -                train: [    INFO] - Test: [   0/62]  Time: 4.768 (4.768)  DataTime: 3.391 (3.391)  Loss:  0.8389 (0.8389)  Acc@1: 86.6250 (86.6250)  Acc@5: 96.1250 (96.1250)
2023-04-10 22:19:19,626 -                train: [    INFO] - Test: [  50/62]  Time: 0.763 (0.822)  DataTime: 0.000 (0.067)  Loss:  1.4180 (1.3193)  Acc@1: 72.7500 (73.9044)  Acc@5: 90.1250 (91.4044)
2023-04-10 22:19:28,416 -                train: [    INFO] - Test: [  62/62]  Time: 0.398 (0.805)  DataTime: 0.000 (0.055)  Loss:  1.1406 (1.3402)  Acc@1: 78.5000 (73.2800)  Acc@5: 92.5000 (91.2720)
2023-04-10 22:19:32,196 -                train: [    INFO] - pop: 0 input: tensor([-1.3644, -1.3644, -1.2617, -1.3987, -1.4500], device='cuda:0')
2023-04-10 22:24:05,519 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
2023-04-10 22:24:05,544 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
2023-04-10 22:24:05,547 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
2023-04-10 22:24:05,602 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
2023-04-10 22:24:05,615 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
2023-04-10 22:24:05,617 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
2023-04-10 22:24:05,628 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
2023-04-10 22:24:05,629 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
2023-04-10 22:24:05,629 - torch.distributed.distributed_c10d: [    INFO] - Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 22:24:05,629 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 3, total 8.
2023-04-10 22:24:05,630 - torch.distributed.distributed_c10d: [    INFO] - Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 22:24:05,630 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 1, total 8.
2023-04-10 22:24:05,631 - torch.distributed.distributed_c10d: [    INFO] - Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 22:24:05,631 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 6, total 8.
2023-04-10 22:24:05,634 - torch.distributed.distributed_c10d: [    INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 22:24:05,634 - torch.distributed.distributed_c10d: [    INFO] - Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 22:24:05,634 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 0, total 8.
2023-04-10 22:24:05,634 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 7, total 8.
2023-04-10 22:24:05,636 - torch.distributed.distributed_c10d: [    INFO] - Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 22:24:05,636 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 5, total 8.
2023-04-10 22:24:05,638 - torch.distributed.distributed_c10d: [    INFO] - Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 22:24:05,638 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 2, total 8.
2023-04-10 22:24:05,639 - torch.distributed.distributed_c10d: [    INFO] - Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 22:24:05,639 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 4, total 8.
2023-04-10 22:24:06,043 -                train: [    INFO] - Model vit_snn created, param count: 29705768
2023-04-10 22:24:06,043 -          data.config: [    INFO] - Data processing configuration for current model + dataset:
2023-04-10 22:24:06,043 -          data.config: [    INFO] - 	input_size: (3, 224, 224)
2023-04-10 22:24:06,043 -          data.config: [    INFO] - 	interpolation: bicubic
2023-04-10 22:24:06,043 -          data.config: [    INFO] - 	mean: (0.485, 0.456, 0.406)
2023-04-10 22:24:06,044 -          data.config: [    INFO] - 	std: (0.229, 0.224, 0.225)
2023-04-10 22:24:06,044 -          data.config: [    INFO] - 	crop_pct: 1.0
2023-04-10 22:24:08,491 -                train: [    INFO] - Using native Torch AMP. Training in mixed precision.
2023-04-10 22:24:08,771 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 22:24:08,827 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-305.pth.tar' (epoch 305)
2023-04-10 22:24:09,165 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 22:24:09,224 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-290.pth.tar' (epoch 290)
2023-04-10 22:24:09,462 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 22:24:09,534 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-298.pth.tar' (epoch 298)
2023-04-10 22:24:09,759 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 22:24:09,828 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-301.pth.tar' (epoch 301)
2023-04-10 22:24:10,037 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 22:24:10,094 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-308.pth.tar' (epoch 308)
2023-04-10 22:24:10,283 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 22:24:10,337 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-285.pth.tar' (epoch 285)
2023-04-10 22:24:10,356 -                train: [    INFO] - Converted model to use Synchronized BatchNorm. WARNING: You may have issues if using zero initialized BN layers (enabled by default for ResNets) while sync-bn enabled.
2023-04-10 22:24:10,356 -                train: [    INFO] - Using native Torch DistributedDataParallel.
2023-04-10 22:24:11,676 -                train: [    INFO] - Scheduled epochs: 20
2023-04-10 22:24:38,423 -                train: [    INFO] - Test: [   0/62]  Time: 4.909 (4.909)  DataTime: 2.270 (2.270)  Loss:  0.7939 (0.7939)  Acc@1: 86.0000 (86.0000)  Acc@5: 97.0000 (97.0000)
2023-04-10 22:25:14,675 -                train: [    INFO] - Test: [  50/62]  Time: 0.688 (0.807)  DataTime: 0.001 (0.045)  Loss:  1.3848 (1.3075)  Acc@1: 72.0000 (73.7525)  Acc@5: 90.1250 (91.4191)
2023-04-10 22:25:23,049 -                train: [    INFO] - Test: [  62/62]  Time: 0.333 (0.786)  DataTime: 0.000 (0.037)  Loss:  1.0898 (1.3271)  Acc@1: 76.5000 (73.2200)  Acc@5: 94.2500 (91.2220)
2023-04-10 22:25:27,690 -                train: [    INFO] - Test: [   0/62]  Time: 4.599 (4.599)  DataTime: 2.954 (2.954)  Loss:  0.8784 (0.8784)  Acc@1: 85.5000 (85.5000)  Acc@5: 95.8750 (95.8750)
2023-04-10 22:26:04,195 -                train: [    INFO] - Test: [  50/62]  Time: 0.635 (0.806)  DataTime: 0.000 (0.059)  Loss:  1.4199 (1.3210)  Acc@1: 72.7500 (73.7549)  Acc@5: 89.7500 (91.3113)
2023-04-10 22:26:11,755 -                train: [    INFO] - Test: [  62/62]  Time: 0.381 (0.772)  DataTime: 0.000 (0.048)  Loss:  1.1436 (1.3439)  Acc@1: 76.5000 (73.0920)  Acc@5: 93.5000 (91.1380)
2023-04-10 22:26:16,206 -                train: [    INFO] - Test: [   0/62]  Time: 4.412 (4.412)  DataTime: 2.639 (2.639)  Loss:  0.8408 (0.8408)  Acc@1: 84.8750 (84.8750)  Acc@5: 96.7500 (96.7500)
2023-04-10 22:26:53,606 -                train: [    INFO] - Test: [  50/62]  Time: 0.753 (0.820)  DataTime: 0.000 (0.053)  Loss:  1.4473 (1.2977)  Acc@1: 71.7500 (73.7353)  Acc@5: 90.0000 (91.3652)
2023-04-10 22:27:02,266 -                train: [    INFO] - Test: [  62/62]  Time: 0.394 (0.801)  DataTime: 0.000 (0.043)  Loss:  1.0605 (1.3181)  Acc@1: 80.0000 (73.2160)  Acc@5: 93.7500 (91.2100)
2023-04-10 22:27:07,003 -                train: [    INFO] - Test: [   0/62]  Time: 4.700 (4.700)  DataTime: 2.546 (2.546)  Loss:  0.8247 (0.8247)  Acc@1: 85.8750 (85.8750)  Acc@5: 95.8750 (95.8750)
2023-04-10 22:27:43,796 -                train: [    INFO] - Test: [  50/62]  Time: 0.634 (0.814)  DataTime: 0.000 (0.051)  Loss:  1.3711 (1.2879)  Acc@1: 72.7500 (73.8922)  Acc@5: 89.5000 (91.4436)
2023-04-10 22:27:51,099 -                train: [    INFO] - Test: [  62/62]  Time: 0.332 (0.775)  DataTime: 0.000 (0.041)  Loss:  1.0811 (1.3065)  Acc@1: 78.2500 (73.3360)  Acc@5: 93.7500 (91.2680)
2023-04-10 22:27:55,832 -                train: [    INFO] - Test: [   0/62]  Time: 4.696 (4.696)  DataTime: 3.280 (3.280)  Loss:  0.8169 (0.8169)  Acc@1: 86.1250 (86.1250)  Acc@5: 97.1250 (97.1250)
2023-04-10 22:28:32,978 -                train: [    INFO] - Test: [  50/62]  Time: 0.632 (0.820)  DataTime: 0.000 (0.065)  Loss:  1.4531 (1.3171)  Acc@1: 70.6250 (73.7451)  Acc@5: 90.2500 (91.3799)
2023-04-10 22:28:40,370 -                train: [    INFO] - Test: [  62/62]  Time: 0.351 (0.781)  DataTime: 0.000 (0.053)  Loss:  1.1445 (1.3394)  Acc@1: 77.2500 (73.1180)  Acc@5: 92.0000 (91.2300)
2023-04-10 22:28:45,334 -                train: [    INFO] - Test: [   0/62]  Time: 4.927 (4.927)  DataTime: 3.305 (3.305)  Loss:  0.8350 (0.8350)  Acc@1: 85.3750 (85.3750)  Acc@5: 97.2500 (97.2500)
2023-04-10 22:29:22,164 -                train: [    INFO] - Test: [  50/62]  Time: 0.749 (0.819)  DataTime: 0.000 (0.066)  Loss:  1.3916 (1.3108)  Acc@1: 72.1250 (73.8652)  Acc@5: 90.7500 (91.3627)
2023-04-10 22:29:29,628 -                train: [    INFO] - Test: [  62/62]  Time: 0.333 (0.781)  DataTime: 0.000 (0.053)  Loss:  1.0654 (1.3288)  Acc@1: 79.5000 (73.3300)  Acc@5: 93.5000 (91.2340)
2023-04-10 22:29:33,957 -                train: [    INFO] - pop: 0 input: tensor([-1.3644, -1.3644, -1.2617, -1.3987, -1.4500], device='cuda:0')
2023-04-10 22:59:52,317 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
2023-04-10 22:59:52,336 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
2023-04-10 22:59:52,342 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
2023-04-10 22:59:52,359 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
2023-04-10 22:59:52,370 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
2023-04-10 22:59:52,389 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
2023-04-10 22:59:52,399 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
2023-04-10 22:59:52,399 - torch.distributed.distributed_c10d: [    INFO] - Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 22:59:52,399 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
2023-04-10 22:59:52,399 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 3, total 8.
2023-04-10 22:59:52,400 - torch.distributed.distributed_c10d: [    INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 22:59:52,400 - torch.distributed.distributed_c10d: [    INFO] - Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 22:59:52,400 - torch.distributed.distributed_c10d: [    INFO] - Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 22:59:52,400 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 0, total 8.
2023-04-10 22:59:52,400 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 1, total 8.
2023-04-10 22:59:52,400 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 5, total 8.
2023-04-10 22:59:52,400 - torch.distributed.distributed_c10d: [    INFO] - Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 22:59:52,401 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 6, total 8.
2023-04-10 22:59:52,401 - torch.distributed.distributed_c10d: [    INFO] - Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 22:59:52,401 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 2, total 8.
2023-04-10 22:59:52,405 - torch.distributed.distributed_c10d: [    INFO] - Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 22:59:52,405 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 4, total 8.
2023-04-10 22:59:52,408 - torch.distributed.distributed_c10d: [    INFO] - Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 22:59:52,409 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 7, total 8.
2023-04-10 22:59:52,823 -                train: [    INFO] - Model vit_snn created, param count: 29705768
2023-04-10 22:59:52,823 -          data.config: [    INFO] - Data processing configuration for current model + dataset:
2023-04-10 22:59:52,823 -          data.config: [    INFO] - 	input_size: (3, 224, 224)
2023-04-10 22:59:52,823 -          data.config: [    INFO] - 	interpolation: bicubic
2023-04-10 22:59:52,824 -          data.config: [    INFO] - 	mean: (0.485, 0.456, 0.406)
2023-04-10 22:59:52,824 -          data.config: [    INFO] - 	std: (0.229, 0.224, 0.225)
2023-04-10 22:59:52,824 -          data.config: [    INFO] - 	crop_pct: 1.0
2023-04-10 22:59:55,334 -                train: [    INFO] - Using native Torch AMP. Training in mixed precision.
2023-04-10 22:59:55,620 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 22:59:55,673 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-305.pth.tar' (epoch 305)
2023-04-10 22:59:56,001 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 22:59:56,054 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-290.pth.tar' (epoch 290)
2023-04-10 22:59:56,233 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 22:59:56,286 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-298.pth.tar' (epoch 298)
2023-04-10 22:59:56,462 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 22:59:56,517 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-301.pth.tar' (epoch 301)
2023-04-10 22:59:56,695 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 22:59:56,748 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-308.pth.tar' (epoch 308)
2023-04-10 22:59:56,927 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 22:59:56,980 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-285.pth.tar' (epoch 285)
2023-04-10 22:59:56,998 -                train: [    INFO] - Converted model to use Synchronized BatchNorm. WARNING: You may have issues if using zero initialized BN layers (enabled by default for ResNets) while sync-bn enabled.
2023-04-10 22:59:56,998 -                train: [    INFO] - Using native Torch DistributedDataParallel.
2023-04-10 22:59:57,984 -                train: [    INFO] - Scheduled epochs: 20
2023-04-10 23:00:18,646 -                train: [    INFO] - pop: 0 input: tensor([-1.3644, -1.3644, -1.2617, -1.3987, -1.4500], device='cuda:0')
2023-04-10 23:11:16,434 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
2023-04-10 23:11:16,453 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
2023-04-10 23:11:16,477 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
2023-04-10 23:11:16,490 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
2023-04-10 23:11:16,510 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
2023-04-10 23:11:16,524 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
2023-04-10 23:11:16,527 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
2023-04-10 23:11:16,532 - torch.distributed.distributed_c10d: [    INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
2023-04-10 23:11:16,533 - torch.distributed.distributed_c10d: [    INFO] - Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 23:11:16,533 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 3, total 8.
2023-04-10 23:11:16,535 - torch.distributed.distributed_c10d: [    INFO] - Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 23:11:16,535 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 2, total 8.
2023-04-10 23:11:16,536 - torch.distributed.distributed_c10d: [    INFO] - Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 23:11:16,536 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 7, total 8.
2023-04-10 23:11:16,537 - torch.distributed.distributed_c10d: [    INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 23:11:16,538 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 0, total 8.
2023-04-10 23:11:16,538 - torch.distributed.distributed_c10d: [    INFO] - Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 23:11:16,538 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 5, total 8.
2023-04-10 23:11:16,539 - torch.distributed.distributed_c10d: [    INFO] - Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 23:11:16,540 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 1, total 8.
2023-04-10 23:11:16,541 - torch.distributed.distributed_c10d: [    INFO] - Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 23:11:16,542 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 6, total 8.
2023-04-10 23:11:16,542 - torch.distributed.distributed_c10d: [    INFO] - Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-04-10 23:11:16,543 -                train: [    INFO] - Training in distributed mode with multiple processes, 1 GPU per process. Process 4, total 8.
2023-04-10 23:11:16,896 -                train: [    INFO] - Model vit_snn created, param count: 29705768
2023-04-10 23:11:16,896 -          data.config: [    INFO] - Data processing configuration for current model + dataset:
2023-04-10 23:11:16,897 -          data.config: [    INFO] - 	input_size: (3, 224, 224)
2023-04-10 23:11:16,897 -          data.config: [    INFO] - 	interpolation: bicubic
2023-04-10 23:11:16,897 -          data.config: [    INFO] - 	mean: (0.485, 0.456, 0.406)
2023-04-10 23:11:16,897 -          data.config: [    INFO] - 	std: (0.229, 0.224, 0.225)
2023-04-10 23:11:16,897 -          data.config: [    INFO] - 	crop_pct: 1.0
2023-04-10 23:11:19,187 -                train: [    INFO] - Using native Torch AMP. Training in mixed precision.
2023-04-10 23:11:19,464 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 23:11:19,517 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-305.pth.tar' (epoch 305)
2023-04-10 23:11:19,831 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 23:11:19,882 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-290.pth.tar' (epoch 290)
2023-04-10 23:11:20,062 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 23:11:20,113 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-298.pth.tar' (epoch 298)
2023-04-10 23:11:20,295 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 23:11:20,346 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-301.pth.tar' (epoch 301)
2023-04-10 23:11:20,531 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 23:11:20,584 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-308.pth.tar' (epoch 308)
2023-04-10 23:11:20,778 -  timm.models.helpers: [    INFO] - Restoring model state from checkpoint...
2023-04-10 23:11:20,830 -  timm.models.helpers: [    INFO] - Loaded checkpoint '/root/desnn/top10-checkpoints/checkpoint-285.pth.tar' (epoch 285)
2023-04-10 23:11:20,849 -                train: [    INFO] - Converted model to use Synchronized BatchNorm. WARNING: You may have issues if using zero initialized BN layers (enabled by default for ResNets) while sync-bn enabled.
2023-04-10 23:11:20,849 -                train: [    INFO] - Using native Torch DistributedDataParallel.
2023-04-10 23:11:22,657 -                train: [    INFO] - Scheduled epochs: 20
2023-04-10 23:11:42,700 -                train: [    INFO] - pop: 0 input: tensor([-1.3644, -1.3644, -1.2617, -1.3987, -1.4500], device='cuda:0')
